{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYBFyybDb9Ns"
      },
      "source": [
        "This notebook is part of 4 approaches. This notebook implements the Entity Extarction by exploiting the coordiantes of token. The final assessment is done by comparing the each token labels and if it 'exactly' matches the actual label then only the prediction is considered correct ( this was done to benchmark it with Google AUTO ML part ). Though the model structure for each entity is same, but they werre trained for each separately for each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCC9hrMEZLJ1",
        "outputId": "021e61a5-bee5-43ec-836b-cbcb867f8a5a"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OvYRjD9PxZj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import operator\n",
        "from tensorflow.keras import backend as be\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.constraints import maxnorm\n",
        "from tensorflow_addons.optimizers import RectifiedAdam\n",
        "from tensorflow.keras.losses import categorical_crossentropy, sparse_categorical_crossentropy, binary_crossentropy\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,Adadelta,RMSprop\n",
        "from tensorflow.keras  import losses\n",
        "from tensorflow.keras import backend as be\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from numpy.random import seed\n",
        "seed(7)\n",
        "tf.compat.v1.set_random_seed(7)\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqbh4OCqP8DS",
        "outputId": "9e3eb7c6-5472-4bf8-9994-90eeb873b119"
      },
      "outputs": [],
      "source": [
        "# Lets mount the Google Drive and acccess the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHrV3fSBPxZl"
      },
      "source": [
        "The code for reading and extracting the information from jsonl file.\n",
        "\n",
        "##### EXTRACTION OF LABELS START OFFSET AND END OFFSET VALUES and TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCYPj5LnPxZm",
        "outputId": "22d5c77b-c28d-42b7-84d3-1e5a89053376"
      },
      "outputs": [],
      "source": [
        "# as the the file type is jsonl, each invoice is represented by hierarchical json object, therefore\n",
        "# we need to fetch each invoice in a hierarchical manner\n",
        "documents = []\n",
        "\n",
        "# each invoice belongs to either train, validation, test set\n",
        "category = []\n",
        "\n",
        "# with the json object of each invoice we also have another json object with key (label), value (string) pair\n",
        "# each annotation(label) has been given one initial 'null' value as there cannot be empty dictionary with keys.\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "\n",
        "\n",
        "# this is similar to placeholder but will read the tokens provided by OCR for each invoice.\n",
        "tokens = {'sentTokens':['null'], 'sentToken_startOffset':['null'], 'sentToken_endOffset':['null'],'boundingPoly':['null']}\n",
        "\n",
        "\n",
        "# this reads the csvfile which gives list of all json files\n",
        "list_ = pd.read_csv('/content/drive/MyDrive/.....csv',usecols=[0,2], names=['Type', 'Filename'], header=None)\n",
        "for type_, file_name in list_.itertuples(index=False):\n",
        "\n",
        "    # read the jsonfile\n",
        "    with open('/content/drive/MyDrive/......./{}'.format(file_name), 'r', encoding=\"utf8\") as json_file:\n",
        "        json_list = list(json_file)\n",
        "\n",
        "    # each invoice is encapsulated in one dictionary with many key-value pairs\n",
        "    for json_str in json_list:\n",
        "        result = json.loads(json_str)\n",
        "\n",
        "\n",
        "        #-----------------READING THE RAW TEXT AND INVOICE TYPE-------------------------------------------\n",
        "        # extracting the raw text provided by OCR \n",
        "        documents.append(result['document']['documentText']['content'])\n",
        "\n",
        "        # extracting the invoice type (train/validation/test) \n",
        "        category.append(type_)\n",
        "\n",
        "        #------------------EXTRACTING OFFSET VALUES FOR LABELS------------------------------------------\n",
        "        # As not all invoices have all the 4 annotations we need to keep track of the same\n",
        "        truth_table = {'Date': False, 'Address': False, 'Name': False, 'Amount':False}\n",
        "\n",
        "        # extracting the annotations and appending them to the placeholder\n",
        "        for i in range(len(result['annotations'])):\n",
        "                placeholder[result['annotations'][i]['displayName']].append(result['annotations'][i]['textExtraction']['textSegment'])\n",
        "                truth_table[result['annotations'][i]['displayName']] = True\n",
        "        \n",
        "        # if any annotation is not available for invoice then append empty value for the same \n",
        "        for key, value in truth_table.items():\n",
        "            if value == False:\n",
        "                placeholder[key].append({})\n",
        "\n",
        "        #--------------------------------EXTRACTING TOKENS GIVEN BY OCR for EACH INVOICE and its bounding Polygon--------------------------\n",
        "        temp = {'tokensOfinvoice':[],'polygons':[]}\n",
        "        start_off = []\n",
        "        end_off = []\n",
        "\n",
        "        # extracting the raw text provided by OCR \n",
        "        for i in range(len(result['document']['layout'])):\n",
        "            temp['tokensOfinvoice'].append(result['document']['layout'][i]['textSegment']['content'])\n",
        "            temp['polygons'].append(result['document']['layout'][i]['boundingPoly']['normalizedVertices'])\n",
        "            if 'startOffset' in result['document']['layout'][i]['textSegment']:\n",
        "                start_off.append(int(result['document']['layout'][i]['textSegment']['startOffset']))\n",
        "            else:\n",
        "                start_off.append(0)\n",
        "            \n",
        "            if 'endOffset' in result['document']['layout'][i]['textSegment']:\n",
        "                end_off.append(int(result['document']['layout'][i]['textSegment']['endOffset']))\n",
        "            else: \n",
        "                end_off.append(-1)\n",
        "\n",
        "        tokens['sentTokens'].append(temp['tokensOfinvoice'])\n",
        "        tokens['boundingPoly'].append(temp['polygons'])\n",
        "        tokens['sentToken_startOffset'].append(start_off)\n",
        "        tokens['sentToken_endOffset'].append(end_off)\n",
        "\n",
        "\n",
        "# getting the list of all annotations offset and tokens ( removing first null entry )\n",
        "placeholder['Date'] = placeholder['Date'][1:]\n",
        "placeholder['Address']= placeholder['Address'][1:]\n",
        "placeholder['Name'] = placeholder['Name'][1:]\n",
        "placeholder['Amount'] = placeholder['Amount'][1:]\n",
        "\n",
        "placeholder['sentTokens'] = tokens['sentTokens'][1:]\n",
        "placeholder['sentToken_startOffset'] = tokens['sentToken_startOffset'][1:]\n",
        "placeholder['sentToken_endOffset'] = tokens['sentToken_endOffset'][1:]\n",
        "placeholder['boundingPoly'] = tokens['boundingPoly'][1:]\n",
        "\n",
        "# appending the raw text and type of invoice\n",
        "placeholder['text'] = documents\n",
        "placeholder['dataType'] = category\n",
        "\n",
        "# checking for the count of all annotations and documents\n",
        "print(placeholder.keys())\n",
        "print(len(placeholder['Amount']))\n",
        "print(len(placeholder['Date']))\n",
        "print(len(placeholder['Address']))\n",
        "print(len(placeholder['Name']))\n",
        "print(len(placeholder['text']))\n",
        "print(len(placeholder['dataType']))\n",
        "\n",
        "data = pd.DataFrame.from_dict(placeholder)\n",
        "\n",
        "# Remember we have dictionary containing the start and end offset, we can extract them \n",
        "# separately too, if required.\n",
        "#--------------------------------------------------------------------------\n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']:\n",
        "    # sometimes startoffset might not be present if annotation start from first index of raw text\n",
        "    data['{}_start'.format(keys)] = data[keys].apply(lambda x: int(x['startOffset']) if 'startOffset' in x.keys()\n",
        "                        else ( 0 if 'endOffset' in x.keys() else None ))\n",
        "\n",
        "    # similarly as above endoffset might not be present if the annotations end is last index of raw text                 \n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']:   \n",
        "    data['{}_end'.format(keys)] = data[keys].apply(lambda x: int(x['endOffset']) if 'endOffset' in x.keys()\n",
        "                        else ( -1 if 'startOffset' in x.keys() else None))\n",
        "\n",
        "data = data.drop(['Date', 'Address', 'Name', 'Amount'], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n1Jcq7nPxZr"
      },
      "source": [
        "Lets contruct the target for the models, the OCR gives the startoffset and endoffset for each label, however this is at character. To make the encoding for each label at the token level, we will use simple logic if the startOffset of token lies within the bound of actual label then its the part of label else not. This approach has a limitation, because if the actual label is sub-part of token then this will exclude that token.This is one limitation of using the output of OCR. This same cause is also the reason for the AutoML's bad performance, because any model will work on token level, however if the labelling is done by taking the sub-span within token then model is not able to take that into factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UhY6-bmPxZs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# placeholder to keep the target of each label\n",
        "import math\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "for i,j in placeholder.items():\n",
        "    for a,b,c,d in zip(data['sentToken_startOffset'],data['sentToken_endOffset'],data['{}_start'.format(i)], data['{}_end'.format(i)]):\n",
        "\n",
        "        if not math.isnan(c):\n",
        "\n",
        "            start_left = np.array(a) >=c\n",
        "            start_right = np.array(a) <=d\n",
        "            start_truth_table = start_left == start_right\n",
        "            \n",
        "            end_left = np.array(b)  >= c\n",
        "            end_right = np.array(b) <=d\n",
        "            end_truth_table = end_left == end_right\n",
        "            truth_table = start_truth_table + end_truth_table \n",
        "\n",
        "            placeholder['{}'.format(i)].append(truth_table.astype(int))\n",
        "        else:\n",
        "            placeholder['{}'.format(i)].append(list(np.zeros(len(a)).astype(int)))\n",
        "\n",
        "placeholder['dateTarget'] = placeholder['Date'][1:]\n",
        "placeholder['addressTarget']= placeholder['Address'][1:]\n",
        "placeholder['nameTarget'] = placeholder['Name'][1:]\n",
        "placeholder['amountTarget'] = placeholder['Amount'][1:]\n",
        "\n",
        "del placeholder['Address'], placeholder['Amount'], placeholder['Date'], placeholder['Name']\n",
        "\n",
        "y = pd.DataFrame.from_dict(placeholder)\n",
        "data = pd.concat([data,y], axis =1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGGTl7B_PxZs",
        "outputId": "46e6e6f3-b0cb-4e6a-e653-ad59456622b3"
      },
      "outputs": [],
      "source": [
        "# If any token is present in label, then the the max value in the list for that sentence for that label will be 1, \n",
        "# hence if eveything is fine then we shoud see same numbers for all labels. However as highlighted due to slicing of token in case of amount, \n",
        "# it is have mis match. \n",
        "# Ex: if the token is RM170.00, now in labelling the actual value of amount is just 170.00, but since this value is sub-span of this whole token, \n",
        "# the encoding cannnot be done easily, therefore we to take care of such cases we take the whole token rather than just the sub-span.\n",
        "\n",
        "print('number of Date labels', sum([max(x) for x in data.dateTarget]))\n",
        "print('number of Name labels', sum([max(x) for x in data.nameTarget]))\n",
        "print('number of Address labels', sum([max(x) for x in data.addressTarget]))\n",
        "print('number of Amount labels', sum([max(x) for x in data.amountTarget]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br4q7TA2PxZt"
      },
      "source": [
        "Before we proceed lets invistigate if everything is okay in terms of data frame creation or not. We will do this by extracting the token from annotations encoding for each label and comparing with the actuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MqX4ZcC0Eo9"
      },
      "outputs": [],
      "source": [
        "## get the tokens set which form part of each label.\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.dateTarget) ]\n",
        "data['dateTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.nameTarget) ]\n",
        "data['nameTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.addressTarget) ]\n",
        "data['addressTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.amountTarget) ]\n",
        "data['amountTokens'] = temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6emR4RyqPxZu",
        "outputId": "4e14bb00-e62f-4c59-aa9b-c46ddd8f12e9"
      },
      "outputs": [],
      "source": [
        "# lets check the  dataframe\n",
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fxzhdf00NPR"
      },
      "outputs": [],
      "source": [
        "## Lets read the actual text which form part of each label\n",
        "# placeholder for annotations text\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "\n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']: \n",
        "    # read annotation_start, annotation_end and text from data\n",
        "    for a,b,c in data[['{}_start'.format(keys), '{}_end'.format(keys), 'text']].itertuples(index=False):\n",
        "        # extracting the text\n",
        "        if not np.isnan(a) and  not np.isnan(b):\n",
        "            placeholder[keys].append(c[int(a):int(b)])\n",
        "        # if annotation not present then append None\n",
        "        else:\n",
        "            placeholder[keys].append(None)\n",
        "\n",
        "placeholder['dateTextactual'] = placeholder['Date'][1:]\n",
        "placeholder['addressTextactual']= placeholder['Address'][1:]\n",
        "placeholder['nameTextactual'] = placeholder['Name'][1:]\n",
        "placeholder['amountTextactual'] = placeholder['Amount'][1:]\n",
        "\n",
        "del placeholder['Address'], placeholder['Amount'], placeholder['Date'], placeholder['Name']\n",
        "\n",
        "y = pd.DataFrame.from_dict(placeholder)\n",
        "\n",
        "###-------Lets construct the each annotations from tokens and targetencoding\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.dateTokens]\n",
        "y['dateFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.nameTokens]\n",
        "y['nameFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.addressTokens]\n",
        "y['addressFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.amountTokens]\n",
        "y['amountFromTokens'] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "oE28fCVC0QZa",
        "outputId": "6aa5d032-1d5d-4128-a756-505f5d18b2cc"
      },
      "outputs": [],
      "source": [
        "# just for demo..purpose\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25U0E_ODmpmK"
      },
      "source": [
        "Lets start building the model now. We will start with feature extraction. For this we will use the spacy to give features values for each token. \n",
        "\n",
        "> \n",
        "*   Feature1: index of token in the invoice\n",
        "*   Feature2: Is token alphanumeric or not\n",
        "*   Feature3: Whether Token is like Num or not ( its different from is-alpha Ex: 01-11189 will have like_Num =  True and is_alpha = False, but the token 21:15 will have like_Num = False and is_alpha = False, as the other is likely representing time and is not purely a Number)\n",
        "*   Feature4: length of token\n",
        "Lets have a look what kind of values we get one invoice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVuoqfKATMag"
      },
      "outputs": [],
      "source": [
        "##lets explore what kind of values we get, and what it means by these features\n",
        "\n",
        "# function to take the list of tokens for each invoice and return the features for each token\n",
        "\n",
        "def invoiceFeatures(sent,polygons): \n",
        "    idx = 0\n",
        "    temp1 = []\n",
        "    temp2 = []\n",
        "    for word, box in zip(sent, polygons):\n",
        "        box[0]\n",
        "        \n",
        "        # as some token dont have x or y coordinate if it start from 0, therefore need to check for that\n",
        "        indices = [ checkMissingCoordinate(box[0]), checkMissingCoordinate(box[1]), checkMissingCoordinate(box[2]), checkMissingCoordinate(box[3])]\n",
        "        temp1.append(indices)\n",
        "        feat = [idx, word.isalpha(), word.isalnum(), len(word)]\n",
        "        temp2.append(feat)\n",
        "        idx +=1\n",
        "    temp1, temp2  = np.array(temp1), np.array(temp2)\n",
        "    return np.concatenate((temp1.reshape(temp1.shape[0],8) , temp2), axis = 1)\n",
        "\n",
        "# function to iterate over the all the invoice, this will call sent2features internally\n",
        "def prep_features(sents,polygonsList):\n",
        "    sent_features = []\n",
        "    i = 0\n",
        "    for sent,polygons in zip(sents, polygonsList):\n",
        "        temp = invoiceFeatures(sent, polygons)\n",
        "        temp = np.transpose(temp)\n",
        "        temp = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len, value = -1, dtype= 'float32')\n",
        "        temp = np.transpose(temp)\n",
        "        # print(temp)\n",
        "        sent_features.append(temp)\n",
        "    return np.array(sent_features)\n",
        "\n",
        "def checkMissingCoordinate(coordinatePair):\n",
        "    temp = []\n",
        "\n",
        "    if 'x' in coordinatePair.keys():\n",
        "        temp.append(coordinatePair['x'])\n",
        "    else:\n",
        "        temp.append(0.0)\n",
        "    \n",
        "    if 'y' in coordinatePair.keys():\n",
        "        temp.append(coordinatePair['y'])\n",
        "    else: \n",
        "        temp.append(0.0)\n",
        "    \n",
        "    return temp\n",
        "\n",
        "# this is just some tokenizing technique using keras and create the library of tokens...\n",
        "# lets build custom tokenizer adapted for this corpus\n",
        "\n",
        "def tokenize(lang, num_words = None):\n",
        "    # lang = list of sentences in a language\n",
        "\n",
        "    # lets select the default config and basic Tokenizer from keras\n",
        "    \n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', num_words= num_words)\n",
        "\n",
        "    # this step is necessary to allow tokenzier build its words id for the corpus \n",
        "    # from the lang = list of sentences\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "    ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "    ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "    ## and pads the sequences to match the longest sequences in the given input\n",
        "    # this will make all sentences of same length\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', value = 0)\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPCU2xthUPsW",
        "outputId": "5f2c1804-858d-420f-c9fb-3d65c70d3824"
      },
      "outputs": [],
      "source": [
        "\n",
        "# getting max length of sequence of tokens for invoices\n",
        "# we need this variable for padding\n",
        "max_len = max([len(x) for x in data.sentTokens])\n",
        "print(\"max length is {}\".format(max_len))\n",
        "\n",
        "# we will be dropping the data where any label is missing, this will be done in training only\n",
        "type_data = ['TRAIN', 'VALIDATION', 'TEST']\n",
        "d_train = data[data.dataType == 'TRAIN']\n",
        "d_train = d_train.dropna(axis = 0, subset= ['Date_start','Name_start','Address_start','Amount_start','Date_end','Address_end','Name_end','Amount_end'])\n",
        "\n",
        "X_train_feat = prep_features(d_train.sentTokens, d_train.boundingPoly)\n",
        "X_valid_feat = prep_features(data[data.dataType == 'VALIDATION'].sentTokens, data[data.dataType == 'VALIDATION'].boundingPoly )\n",
        "X_test_feat = prep_features(data[data.dataType == 'TEST'].sentTokens, data[data.dataType == 'TEST'].boundingPoly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJwaB7Poc2Z8"
      },
      "outputs": [],
      "source": [
        "# getting token ids for each token and padding each invoice\n",
        "# Further we split the data (train, validation and test)\n",
        "# a,tokenizer_ = tokenize(data.sentTokens)\n",
        "\n",
        "# splitting the data\n",
        "X_train, tokenizer_ = tokenize(d_train.sentTokens)\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len, padding='post', value = 0)\n",
        "\n",
        "X_valid = tokenizer_.texts_to_sequences(data[data.dataType == 'VALIDATION'].sentTokens)\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=max_len, padding='post', value = 0)\n",
        "\n",
        "X_test = tokenizer_.texts_to_sequences(data[data.dataType == 'TEST'].sentTokens)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len, padding='post', value = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPcohEP0QSz1"
      },
      "outputs": [],
      "source": [
        "# Conv2D layers will will need the 3Dimensions apart from batch size ( fourth dimension is morelike channel)\n",
        "\n",
        "X_train_feat = np.expand_dims(X_train_feat,-1)\n",
        "X_valid_feat = np.expand_dims(X_valid_feat,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEeHWtoxh5lv",
        "outputId": "e71b9445-14a4-42a0-8481-5b2acf37ee96"
      },
      "outputs": [],
      "source": [
        "# lets have a look at the shapes of data\n",
        "\n",
        "print(\"X_TRAIN\",X_train.shape)\n",
        "print(\"Train_features\",X_train_feat.shape)\n",
        "print(\"X_VALID\",X_valid.shape)\n",
        "print(\"Valid_features\",X_valid_feat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQzH8cYy4SkA"
      },
      "outputs": [],
      "source": [
        "# this is required as to tell the input shape to model.\n",
        "shape_ = X_train_feat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpKUOyItzKZc"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "  # we will create two inputs one which has sequence of just tokens and\n",
        "  # other which has feature of each token (so array of features of each token)\n",
        "  x1 = Input(shape=(max_len,))\n",
        "  x2 = Input(shape = (max_len,X_train_feat.shape[2],1))\n",
        "#----------------------------------------TOKEN LSTM -----------------------------------\n",
        "\n",
        "  mask1 = layers.Masking(mask_value=0)(x1)\n",
        "  embed = layers.Embedding(input_dim= len(tokenizer_.word_index), output_dim=128, input_length=max_len)(mask1)\n",
        "\n",
        "\n",
        "#---------------------------------------------FEATURE CONVOLUTION-----------------------------\n",
        "\n",
        "  mask2 = layers.Masking(mask_value=-1)(x2)\n",
        "  conv1 = layers.Conv2D(filters = 64, kernel_size= 4, strides = 1, padding= \"same\", activation = 'relu')(mask2)\n",
        "  conv2 = layers.Conv2D(filters = 32, kernel_size= 4, strides = 1, padding= \"same\", activation = 'relu')(conv1)\n",
        "  reshape1 = layers.Reshape((317,shape_[2]*32), input_shape = (317,shape_[2],32))(conv2)\n",
        "  # you can add more layers and make network more complex here\n",
        "\n",
        "\n",
        "#----------------------------------------------Merging both---NOW BILSTM starts-------------------\n",
        "\n",
        "  merge = layers.concatenate([embed,reshape1])\n",
        "  drop1 = layers.Dropout(0.5)(merge)\n",
        "\n",
        "  lstm1 = layers.LSTM(units= 200, return_sequences= True, dropout= 0.4)(drop1)\n",
        "  bilstm1 = layers.Bidirectional(LSTM(units= 200, return_sequences= True, dropout = 0.2))(lstm1)\n",
        "  bilstm2 = layers.Bidirectional(LSTM(units= 100, return_sequences= True))(bilstm1)\n",
        "  norm1 = layers.BatchNormalization(axis = -1)(bilstm2)\n",
        "  # you can add more layers and make model more complex.\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "  # KEEP THIS INTACT\n",
        "  predictions = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(norm1)\n",
        "\n",
        "  model = Model(inputs = ([x1,x2]), outputs = predictions)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiAuDs9B1ua7"
      },
      "outputs": [],
      "source": [
        "# function to plot the loss, precision and recall, \n",
        "# caution code block has lines to save the files to drive.\n",
        "\n",
        "def display_history(history,name =\"None\"):\n",
        "\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import numpy as np                                        \n",
        "    # \"\"\"Summarize history for accuracy and loss.\n",
        "\n",
        "    sns.set_palette(\"pastel\")\n",
        "    sns.set(style=\"darkgrid\")\n",
        "    path = '/content/drive/MyDrive/..../'\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Loss_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['loss'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_loss'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('loss')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig(path + title + '_coordinates.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Precision_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['precision'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_precision'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('precision')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig( path + title + '_coordinates.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Recall_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['recall'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_recall'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('recall')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig( path + title + '_coordinates.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6tTodA62S80"
      },
      "outputs": [],
      "source": [
        "def f1_metric(y_true, y_pred):\n",
        "    true_positives = be.sum(be.round(be.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = be.sum(be.round(be.clip(y_true, 0, 1)))\n",
        "    predicted_positives = be.sum(be.round(be.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + be.epsilon())\n",
        "    recall = true_positives / (possible_positives + be.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+be.epsilon())\n",
        "    \n",
        "    return f1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZYQAIWkRuyT"
      },
      "outputs": [],
      "source": [
        "# this will take the tag = 'date'/'name'/'address'.....make the target encoding as per that and load and compile the model\n",
        "def LoadandCompile(tag):\n",
        "  be.clear_session()\n",
        "  temp = d_train['{}Target'.format(tag)].tolist()\n",
        "  y_train = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "  y_train = np.array(y_train)\n",
        "\n",
        "  temp = data[data.dataType == 'VALIDATION']['{}Target'.format(tag)].tolist()\n",
        "  y_valid = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "  y_valid = np.array(y_valid)\n",
        "\n",
        "  # this is cutom entity so we can use binary cross entropy.\n",
        "  loss = binary_crossentropy\n",
        "  optimizer=RectifiedAdam(learning_rate = 0.001, warmup_proportion = 0.2, beta_1 = 0.9, \n",
        "                                            total_steps= 2000, min_lr= 0.0005)\n",
        "  model = build_model()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss= loss,\n",
        "                metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(),tf.keras.metrics.Recall(), f1_metric])\n",
        "  \n",
        "  return model, y_train, y_valid  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpWMbGrx3XXJ"
      },
      "outputs": [],
      "source": [
        "# dataframe for bookkeeping\n",
        "historys = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3aMwnYCdXQAq",
        "outputId": "ec075689-eb7a-47c0-9afe-73cca95e05dd"
      },
      "outputs": [],
      "source": [
        "# lets run the model for each annotation\n",
        "annotations  = ['date', 'name', 'address', 'amount']\n",
        "\n",
        "for i in annotations:\n",
        "  be.clear_session()\n",
        "  model,y_train, y_valid = LoadandCompile(i)\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_metric',patience= 25,\n",
        "                                                   mode='max', restore_best_weights = True)\n",
        "\n",
        "  print(\"Starting training {} on with model presented above\".format(i))\n",
        "  history = model.fit(x=[X_train,X_train_feat], y=y_train,\n",
        "                    validation_data=([X_valid,X_valid_feat], y_valid),\n",
        "                    epochs=300,batch_size=32,  callbacks = [early_stopping])\n",
        "  \n",
        "  # saving the model\n",
        "\n",
        "  model.save('/content/drive/MyDrive/...../{}_cooordinates_earlystopping.h5'.format(i))\n",
        "  val_loss, val_acc, val_prec, val_recall, val_F1 = model.evaluate([X_valid,X_valid_feat],y_valid)\n",
        "  historys.append([i, val_loss,val_acc,val_prec,val_recall,val_F1])\n",
        "  display_history(history, name = i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RBcWElBTFp4P",
        "outputId": "46b2be85-24e4-4131-afd3-4235b197df40"
      },
      "outputs": [],
      "source": [
        "annotations  = ['amount']\n",
        "\n",
        "for i in annotations:\n",
        "  be.clear_session()\n",
        "  model,y_train, y_valid = LoadandCompile(i)\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_metric',patience= 40,\n",
        "                                                   mode='max', restore_best_weights = True)\n",
        "  # mcp_save = tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/...../{}_customweight.h5'.format(i), save_best_only=True, monitor='val_precision', mode='max')\n",
        "  # plot_model(model,to_file='model_plot.png',show_shapes = True, show_layer_names = True)\n",
        "  print(\"Starting training {} on with model presented above\".format(i))\n",
        "  history = model.fit(x=[X_train,X_train_feat], y=y_train,\n",
        "                    validation_data=([X_valid,X_valid_feat], y_valid),\n",
        "                    epochs=300,batch_size=16,  callbacks = [early_stopping])\n",
        "  model.save('/content/drive/MyDrive/...../{}_cooordinates_earlystopping.h5'.format(i))\n",
        "  val_loss, val_acc, val_prec, val_recall, val_F1 = model.evaluate([X_valid,X_valid_feat],y_valid)\n",
        "  historys.append([i, val_loss,val_acc,val_prec,val_recall,val_F1])\n",
        "  display_history(history, name = i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SaFzu0T-bUF"
      },
      "outputs": [],
      "source": [
        "# we ahve got the precision and recall during the trianing but that is at token level, \n",
        "# we need now to see the metrics at invoice level as thats the main problem we are solving.\n",
        "\n",
        "def invoiceMetrices(y_pred, tag, df, threshold):\n",
        "  df = df.reset_index()\n",
        "  y_pred  = (y_pred> threshold).astype(int)\n",
        "  predicted ={}\n",
        "  predicted['{}'.format(tag)] = y_pred\n",
        "  predicted_df = {'{}'.format(tag):['null']}\n",
        "\n",
        "  for a,b in zip(df.sentTokens, predicted['{}'.format(tag)]):\n",
        "      b = b.flatten()\n",
        "      temp = [item for item in b[:len(a)]]\n",
        "      predicted_df['{}'.format(tag)].append(temp)\n",
        "  predicted_df['{}'.format(tag)] = predicted_df['{}'.format(tag)][1:]\n",
        "  predicted_df = pd.DataFrame.from_dict(predicted_df)\n",
        "  df['predicted_{}_indexs'.format(tag)] = predicted_df['{}'.format(tag)]\n",
        "\n",
        "  TP = 0\n",
        "  for a,b in zip(df['{}Target'.format(tag)],df['predicted_{}_indexs'.format(tag)]):\n",
        "    count = (a == b)\n",
        "    if type(count) == type(True):\n",
        "      TP += 1\n",
        "    elif sum(count) == len(a):\n",
        "      TP += 1\n",
        "  return (TP/len(y_pred))*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uQbTnJd-k2-",
        "outputId": "159db24b-892b-4e87-f6b6-9ddbc29ae963"
      },
      "outputs": [],
      "source": [
        "# book keeping\n",
        "report = {'Label':[], 'Accuracy':[], 'Threshold': []}\n",
        "\n",
        "# making test model ready\n",
        "X_test_feat = np.expand_dims(X_test_feat,-1)\n",
        "print(X_test.shape)\n",
        "print(X_test_feat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0NfIBB7-uYC"
      },
      "outputs": [],
      "source": [
        "# getting the final metrics for each annotations\n",
        "\n",
        "annotations  = ['amount','date','name','address']\n",
        "\n",
        "threshold = [0.3, 0.35, 0.4,0.45, 0.5, 0.55, 0.6, 0.65]\n",
        "for tag in annotations:\n",
        "  model = tf.keras.models.load_model('/content/drive/MyDrive/....../{}_cooordinates_earlystopping.h5'.format(tag), compile = False)   # custom_objects={ 'metrics': f1_metric(y_valid, ) })\n",
        "  for confidencescore in threshold:\n",
        "    \n",
        "    temp = data[data.dataType == 'TEST']['{}Target'.format(tag)].tolist()\n",
        "    y_test = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "    y_test = np.array(y_test)\n",
        "    y_pred = model.predict([X_test,X_test_feat])\n",
        "    acc = invoiceMetrices(y_pred,tag,data[data.dataType == 'TEST'],confidencescore)\n",
        "    report['Label'].append(tag)\n",
        "    report['Accuracy'].append(acc)\n",
        "    report['Threshold'].append(confidencescore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KEzwwYOj-27T",
        "outputId": "0a0ba7c0-08c5-4c15-adb9-9e70249c95a5"
      },
      "outputs": [],
      "source": [
        "report = pd.DataFrame.from_dict(report)\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVaWHdjZMPOk"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "path = '/content/drive/MyDrive/...../'  # path to the folder in google drive where data is saved\n",
        "with open(path + \"X_train_feat.ob\", 'wb') as a, open(path + \"X_valid_feat.ob\", 'wb') as b, open(path + \"X_test_feat.ob\", 'wb') as c:\n",
        "    pickle.dump(X_train_feat, a) \n",
        "    pickle.dump(X_valid_feat, b)\n",
        "    pickle.dump(X_test_feat, c)\n",
        "\n",
        "\n",
        "with open(path + \"tokenizer.ob\", 'wb') as a, open(path + \"Coordinatesresults.ob\", 'wb') as b, open(path + \"data.ob\", 'wb') as c :\n",
        "  pickle.dump(tokenizer_, a)\n",
        "  pickle.dump(report, b)\n",
        "  pickle.dump(data, c)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Coordinates.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3472910ebd2aa62b6c54b483bd66ffdd88459b69fe7bc051f28b07b9e0034dc5"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('tf_keras': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
