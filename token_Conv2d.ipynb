{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmLILCfsaWxz"
      },
      "source": [
        "About Project:\n",
        "This is Entity Extraction from the semi-structured data (form type). The data was provided in pdf and jpegs format, which was processed through the OCR then the manual part of labelling was carried out by team. Once the baselines had been set using Google NLP product, the custom models were deployed to solve the problem.\n",
        "\n",
        "This notebook is part of 4 approaches. This notebook implements the Entity Extarction by exploiting the features of token. The final assessment is done by comparing the each token labels and if it 'exactly' matches the actual label then only the prediction is considered correct ( this was done to benchmark it with Google AUTO ML part ). Though the model structure for each entity is same, but they werre trained for each separately for each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCC9hrMEZLJ1",
        "outputId": "fec887b6-db78-42ae-f150-d44092b9669f"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OvYRjD9PxZj"
      },
      "outputs": [],
      "source": [
        "# installing necessary notebooks\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import operator\n",
        "from tensorflow.keras import backend as be\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.constraints import maxnorm\n",
        "from tensorflow_addons.optimizers import RectifiedAdam\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,Adadelta,RMSprop\n",
        "from tensorflow.keras  import losses\n",
        "from tensorflow.keras import backend as be\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from numpy.random import seed\n",
        "seed(7)\n",
        "tf.compat.v1.set_random_seed(7)\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqbh4OCqP8DS",
        "outputId": "93ca66a9-eee0-4fbb-a908-ac4ef807f0bc"
      },
      "outputs": [],
      "source": [
        "# Lets mount the Google Drive and acccess the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHrV3fSBPxZl"
      },
      "source": [
        "The code for reading and extracting the information from jsonl file.\n",
        "\n",
        "##### EXTRACTION OF LABELS START OFFSET AND END OFFSET VALUES and TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCYPj5LnPxZm",
        "outputId": "352d6f48-87c3-49e6-a8b8-2524adb2316d"
      },
      "outputs": [],
      "source": [
        "# as the the file type is jsonl, each invoice is represented by hierarchial json object, therefore\n",
        "# we need to fetch each invoice in hierarchial manner\n",
        "# further this will access the folder from where to read the fiels, so please change the path \n",
        "# as per your specifications.\n",
        "documents = []\n",
        "\n",
        "# each invoice belogs to either train, validation, test set\n",
        "category = []\n",
        "\n",
        "# with the json object of each invoice we also have another json object with key (label), value (string) pair\n",
        "# each annotation(label) has been given one initial 'null' value as there cannot be empty dictionary with keys.\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "\n",
        "\n",
        "# this is similar to placeholder but will read the tokens provided by OCR for each invoice.\n",
        "tokens = {'sentTokens':['null'], 'sentToken_startOffset':['null'], 'sentToken_endOffset':['null'],'boundingPoly':['null']}\n",
        "\n",
        "\n",
        "# this reads the csvfile which gives list of all json files\n",
        "list_ = pd.read_csv('/content/drive/MyDrive/..........csv',usecols=[0,2], names=['Type', 'Filename'], header=None)\n",
        "for type_, file_name in list_.itertuples(index=False):\n",
        "\n",
        "    # read the jsonfile\n",
        "    with open('/content/drive/MyDrive/........./{}'.format(file_name), 'r', encoding=\"utf8\") as json_file:\n",
        "        json_list = list(json_file)\n",
        "\n",
        "    # each invoice is encapsulated in one dictionary with many key-value pairs\n",
        "    for json_str in json_list:\n",
        "        result = json.loads(json_str)\n",
        "\n",
        "\n",
        "        #-----------------READING THE RAW TEXT AND INVOICE TYPE-------------------------------------------\n",
        "        # extracting the raw text provided by OCR \n",
        "        documents.append(result['document']['documentText']['content'])\n",
        "\n",
        "        # extracting the invoice type (train/validation/test) \n",
        "        category.append(type_)\n",
        "\n",
        "        #------------------EXTRACTING OFFSET VALUES FOR LABELS------------------------------------------\n",
        "        # As not all invoices has all the 4 annotations we need to keep track of same\n",
        "        truth_table = {'Date': False, 'Address': False, 'Name': False, 'Amount':False}\n",
        "\n",
        "        # extracting the annotations and appending them to the placeholder\n",
        "        for i in range(len(result['annotations'])):\n",
        "                placeholder[result['annotations'][i]['displayName']].append(result['annotations'][i]['textExtraction']['textSegment'])\n",
        "                truth_table[result['annotations'][i]['displayName']] = True\n",
        "        \n",
        "        # if any annotation not available for invoice them append empty value for same \n",
        "        for key, value in truth_table.items():\n",
        "            if value == False:\n",
        "                placeholder[key].append({})\n",
        "\n",
        "        #--------------------------------EXTRACTING TOKENS GIVEN BY OCR for EACH INVOICE and its bounding Polygon--------------------------\n",
        "        temp = {'tokensOfinvoice':[],'polygons':[]}\n",
        "        start_off = []\n",
        "        end_off = []\n",
        "\n",
        "        # extracting the raw text provided by OCR \n",
        "        for i in range(len(result['document']['layout'])):\n",
        "            temp['tokensOfinvoice'].append(result['document']['layout'][i]['textSegment']['content'])\n",
        "            temp['polygons'].append(result['document']['layout'][i]['boundingPoly']['normalizedVertices'])\n",
        "            if 'startOffset' in result['document']['layout'][i]['textSegment']:\n",
        "                start_off.append(int(result['document']['layout'][i]['textSegment']['startOffset']))\n",
        "            else:\n",
        "                start_off.append(0)\n",
        "            \n",
        "            if 'endOffset' in result['document']['layout'][i]['textSegment']:\n",
        "                end_off.append(int(result['document']['layout'][i]['textSegment']['endOffset']))\n",
        "            else: \n",
        "                end_off.append(-1)\n",
        "\n",
        "        tokens['sentTokens'].append(temp['tokensOfinvoice'])\n",
        "        tokens['boundingPoly'].append(temp['polygons'])\n",
        "        tokens['sentToken_startOffset'].append(start_off)\n",
        "        tokens['sentToken_endOffset'].append(end_off)\n",
        "\n",
        "\n",
        "# getting the list of all annotations offset ,tokens and respective bounding polygons ( removing first null entry )\n",
        "placeholder['Date'] = placeholder['Date'][1:]\n",
        "placeholder['Address']= placeholder['Address'][1:]\n",
        "placeholder['Name'] = placeholder['Name'][1:]\n",
        "placeholder['Amount'] = placeholder['Amount'][1:]\n",
        "\n",
        "placeholder['sentTokens'] = tokens['sentTokens'][1:]\n",
        "placeholder['sentToken_startOffset'] = tokens['sentToken_startOffset'][1:]\n",
        "placeholder['sentToken_endOffset'] = tokens['sentToken_endOffset'][1:]\n",
        "placeholder['boundingPoly'] = tokens['boundingPoly'][1:]\n",
        "\n",
        "# appending the raw text and type of invoice\n",
        "placeholder['text'] = documents\n",
        "placeholder['dataType'] = category\n",
        "\n",
        "\n",
        "# checking for count of all annotations and documents\n",
        "print(placeholder.keys())\n",
        "print(len(placeholder['Amount']))\n",
        "print(len(placeholder['Date']))\n",
        "print(len(placeholder['Address']))\n",
        "print(len(placeholder['Name']))\n",
        "print(len(placeholder['text']))\n",
        "print(len(placeholder['dataType']))\n",
        "\n",
        "data = pd.DataFrame.from_dict(placeholder)\n",
        "\n",
        "# Remember we have dictionary containing the start and end offset, we can extractt them \n",
        "# separately too, if required.\n",
        "#--------------------------------------------------------------------------\n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']:\n",
        "    # sometimes startoffset might not be present if annotation start from first index of raw text\n",
        "    data['{}_start'.format(keys)] = data[keys].apply(lambda x: int(x['startOffset']) if 'startOffset' in x.keys()\n",
        "                        else ( 0 if 'endOffset' in x.keys() else None ))\n",
        "\n",
        "    # similarly as above endoffset might not be present if the annotations end is last index of raw text                 \n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']:   \n",
        "    data['{}_end'.format(keys)] = data[keys].apply(lambda x: int(x['endOffset']) if 'endOffset' in x.keys()\n",
        "                        else ( -1 if 'startOffset' in x.keys() else None))\n",
        "\n",
        "data = data.drop(['Date', 'Address', 'Name', 'Amount'], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n1Jcq7nPxZr"
      },
      "source": [
        "Lets contruct the target for the models, the OCR gives the startoffset and endoffset for each label, however this is at a character level. To make the encoding for each label at token level, we will use simple logic if the startOffset or endoffset of token lies within the bounds of actual annotation (start and end offset) then its part of annotation else not. This approach has limitation, because if the actual label is sub-part of token then this will can make model to learn wrong things, however given we are using tokens given by OCR this cant be avoided.This is one limitation of using the output of OCR. This same cause is also the reason for the AutoML bad performance, because any model will work on token level, however if the labelling is done by taking the sub-span within token then model is not able to take that into factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UhY6-bmPxZs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# placeholder to keep the target of each label\n",
        "import math\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "for i,j in placeholder.items():\n",
        "    for a,b,c,d in zip(data['sentToken_startOffset'],data['sentToken_endOffset'],data['{}_start'.format(i)], data['{}_end'.format(i)]):\n",
        "\n",
        "        if not math.isnan(c):\n",
        "            \n",
        "            start_left = np.array(a) >=c\n",
        "            start_right = np.array(a) <=d\n",
        "            start_truth_table = start_left == start_right\n",
        "            \n",
        "            end_left = np.array(b)  >= c\n",
        "            end_right = np.array(b) <=d\n",
        "            end_truth_table = end_left == end_right\n",
        "            truth_table = start_truth_table + end_truth_table \n",
        "\n",
        "            placeholder['{}'.format(i)].append(truth_table.astype(int))\n",
        "        else:\n",
        "            placeholder['{}'.format(i)].append(list(np.zeros(len(a)).astype(int)))\n",
        "\n",
        "# removing the first null entry.\n",
        "placeholder['dateTarget'] = placeholder['Date'][1:]\n",
        "placeholder['addressTarget']= placeholder['Address'][1:]\n",
        "placeholder['nameTarget'] = placeholder['Name'][1:]\n",
        "placeholder['amountTarget'] = placeholder['Amount'][1:]\n",
        "\n",
        "del placeholder['Address'], placeholder['Amount'], placeholder['Date'], placeholder['Name']\n",
        "# creating final dataframe \n",
        "y = pd.DataFrame.from_dict(placeholder)\n",
        "data = pd.concat([data,y], axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGGTl7B_PxZs",
        "outputId": "40898854-272d-4090-8245-3fec6002479a"
      },
      "outputs": [],
      "source": [
        "# If any token is present in label, then the the max value in the list for that sentence for that label will be 1, \n",
        "# hence if eveything is fine then we shoud see same numbers for all labels. However as highlighted due to slicing of token in case of amount, \n",
        "# it is have mis match. \n",
        "# Ex: if the token is RM170.00, now in labelling the actual value of amount is just 170.00, but since this value is sub-span of this whole token, \n",
        "# the encoding cannnot be done eeasily, therefore we to take care of such cases we take the whole token rather than just the sub-span.\n",
        "\n",
        "print('number of Date labels', sum([max(x) for x in data.dateTarget]))\n",
        "print('number of Name labels', sum([max(x) for x in data.nameTarget]))\n",
        "print('number of Address labels', sum([max(x) for x in data.addressTarget]))\n",
        "print('number of Amount labels', sum([max(x) for x in data.amountTarget]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br4q7TA2PxZt"
      },
      "source": [
        "Before we proceed lets invistigate if everything is okay in terms of data frame creation or not. We will do this by extracting the token from annotations encoding for each label and comparing with the actuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPiaIklhPxZt"
      },
      "outputs": [],
      "source": [
        "## get the tokens set which form part of each label.\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.dateTarget) ]\n",
        "data['dateTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.nameTarget) ]\n",
        "data['nameTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.addressTarget) ]\n",
        "data['addressTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.amountTarget) ]\n",
        "data['amountTokens'] = temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6emR4RyqPxZu",
        "outputId": "a3d68a27-5058-409d-c036-6b1162eed5d1"
      },
      "outputs": [],
      "source": [
        "# lets check the whole dataframe\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifKuRZo4QdQa"
      },
      "source": [
        "This is just for demonstration purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmw3GOkCPxZu"
      },
      "outputs": [],
      "source": [
        "## Lets read the actual text which form part of each label\n",
        "# placeholder for annotations text\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "\n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']: \n",
        "    # read annotation_start, annotation_end and text from data\n",
        "    for a,b,c in data[['{}_start'.format(keys), '{}_end'.format(keys), 'text']].itertuples(index=False):\n",
        "        # extracting the text\n",
        "        if not np.isnan(a) and  not np.isnan(b):\n",
        "            placeholder[keys].append(c[int(a):int(b)])\n",
        "        # if annotation not present then append None\n",
        "        else:\n",
        "            placeholder[keys].append(None)\n",
        "\n",
        "placeholder['dateTextactual'] = placeholder['Date'][1:]\n",
        "placeholder['addressTextactual']= placeholder['Address'][1:]\n",
        "placeholder['nameTextactual'] = placeholder['Name'][1:]\n",
        "placeholder['amountTextactual'] = placeholder['Amount'][1:]\n",
        "\n",
        "del placeholder['Address'], placeholder['Amount'], placeholder['Date'], placeholder['Name']\n",
        "\n",
        "y = pd.DataFrame.from_dict(placeholder)\n",
        "\n",
        "###-------Lets construct the each annotations from tokens and targetencoding\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.dateTokens]\n",
        "y['dateFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.nameTokens]\n",
        "y['nameFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.addressTokens]\n",
        "y['addressFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.amountTokens]\n",
        "y['amountFromTokens'] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "fEyMAyZzPxZu",
        "outputId": "93c291d2-b2d4-4ccf-ec6a-8570de1f0836"
      },
      "outputs": [],
      "source": [
        "# just for demo..purpose\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr-rNpN4Qryg"
      },
      "source": [
        "Lets start building the model now. We will start with feature extraction. For this we will use the spacy to give features values for each token. \n",
        "\n",
        "> \n",
        "*   Feature1: index of token in the invoice\n",
        "*   Feature2: POS of token, usually this is used in proper sentences only, however we will try to see if even getting POS for single token like whether it is (NOUN, PNOUN  etc helps or not)\n",
        "*   Feature3: Shape type of token\n",
        "*   Feature4: Whether Token is like Num or not ( its different from is-alpha Ex: 01-11189 will have like_Num =  True and is_alpha = False, but the token 21:15 will have like_Num = False and is_alpha = False, as the other is likely representing time and is not purely a Number)\n",
        "*   Feature5: Is token alphanumeric or not\n",
        "\n",
        "Lets have a look what kind of values we get one invoice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26VjtJ7eUosp"
      },
      "outputs": [],
      "source": [
        "# creating nlp object for getting the features of each token.\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WGlPwNydUei9",
        "outputId": "67daf9cc-15a3-4664-d2a1-79b7cc7886e6"
      },
      "outputs": [],
      "source": [
        "# demonstration of features\n",
        "temp = []\n",
        "for word in data.sentTokens[0]:\n",
        "  token = nlp(word)\n",
        "  temp.append([word,token[0].pos_, token[0].shape_ , token[0].like_num, token[0].is_alpha])\n",
        "\n",
        "df = pd.DataFrame(temp, columns=['Word','POS','SHAPE','LIKE_NUM','IS_ALPHANUMERIC'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVuoqfKATMag"
      },
      "outputs": [],
      "source": [
        "\n",
        "# function to take the list of tokens for each invoice and return the features for each token\n",
        "\n",
        "def sent2features(sent,posdict,shapedict):\n",
        "    countOfPos = len(posdict)\n",
        "    countOfShape = len(shapedict)\n",
        "    \n",
        "    idx = 0\n",
        "    temp = []\n",
        "    for word in sent:        \n",
        "        token = nlp(word)\n",
        "        if token[0].pos_ not in posdict:\n",
        "            posdict[token[0].pos_] = countOfPos\n",
        "            countOfPos += 1\n",
        "        if token[0].shape_ not in shapedict:\n",
        "            shapedict[token[0].shape_] = countOfShape\n",
        "            countOfShape += 1\n",
        "\n",
        "        # we need the ids of each feature rather than feature value \n",
        "        temp.append([idx, posdict[token[0].pos_], shapedict[token[0].shape_], token[0].like_num, token[0].is_alpha]) \n",
        "        idx +=1\n",
        "\n",
        "    return np.array(temp), posdict, shapedict\n",
        "\n",
        "# function to iterate over the all invoice, this will call sent2features internally\n",
        "def prep_features(sents,posdict ={},shapedict={}):\n",
        "    sent_features = []\n",
        "    for sent in sents:\n",
        "        temp, posdict, shapedict = sent2features(sent, posdict, shapedict)\n",
        "    \n",
        "        temp = np.transpose(temp)\n",
        "        # we need each entry of same size, thereofre doing padding\n",
        "        temp = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len, value = -1)\n",
        "        temp = np.transpose(temp)\n",
        "        sent_features.append(temp)\n",
        "\n",
        "    return np.array(sent_features), posdict,shapedict\n",
        "\n",
        "\n",
        "# this is just some tokenizing technique using keras and create the library of tokens...\n",
        "# lets build custom tokenizer adapted for this corpus\n",
        "\n",
        "def tokenize(lang, num_words = None):\n",
        "    # lang = list of sentences in a language\n",
        "    # lets select the default config and basic Tokenizer from keras\n",
        "    \n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', num_words= num_words)\n",
        "\n",
        "    # this step is necessary to allow the tokenzier to build its words id for the corpus \n",
        "    # from the lang = list of sentences\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "    ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "    ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "    ## and pads the sequences to match the longest sequences in the given input\n",
        "    # this will make all sentences of same length\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', value = 0)\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvnAAEPiSONz"
      },
      "source": [
        "We will create two array one as features of each token described above and other for the list of token ( the token id will be provided by tokenizer defined above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPCU2xthUPsW",
        "outputId": "33932c96-d42c-400f-9cf1-3ef7cdd4bdb6"
      },
      "outputs": [],
      "source": [
        "posdict = {}\n",
        "shapedict = {}\n",
        "\n",
        "# getting max length of sequence of tokens for invoices\n",
        "# we need this variable for padding\n",
        "max_len = max([len(x) for x in data.sentTokens])\n",
        "print(\"max length is {}\".format(max_len))\n",
        "\n",
        "# we will be dropping the data where any label is missing, this will be done in training\n",
        "type_data = ['TRAIN', 'VALIDATION', 'TEST']\n",
        "d_train = data[data.dataType == 'TRAIN']\n",
        "d_train = d_train.dropna(axis = 0, subset= ['Date_start','Name_start','Address_start','Amount_start','Date_end','Address_end','Name_end','Amount_end'])\n",
        "\n",
        "X_train_feat, posdict, shapedict = prep_features(d_train.sentTokens)\n",
        "X_valid_feat, posdict, shapedict = prep_features(data[data.dataType == 'VALIDATION'].sentTokens ,posdict,shapedict )\n",
        "X_test_feat,posdict, shapedict = prep_features(data[data.dataType == 'TEST'].sentTokens , posdict, shapedict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJwaB7Poc2Z8"
      },
      "outputs": [],
      "source": [
        "# getting token ids for each token and padding each invoice\n",
        "# Further we split the data (train, validation and test)\n",
        "\n",
        "# splitting the data\n",
        "X_train, tokenizer_ = tokenize(d_train.sentTokens)\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len, padding='post', value = 0)\n",
        "\n",
        "X_valid = tokenizer_.texts_to_sequences(data[data.dataType == 'VALIDATION'].sentTokens)\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=max_len, padding='post', value = 0)\n",
        "\n",
        "X_test = tokenizer_.texts_to_sequences(data[data.dataType == 'TEST'].sentTokens)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len, padding='post', value = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPcohEP0QSz1"
      },
      "outputs": [],
      "source": [
        "# Conv2D layers will will need the 3Dimensions apart from batch size ( fourth dimension is morelike channel)\n",
        "X_train_feat= np.expand_dims(X_train_feat,-1)\n",
        "X_valid_feat= np.expand_dims(X_valid_feat,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEeHWtoxh5lv",
        "outputId": "c9dea6ef-f787-471e-d62a-4923098fa292"
      },
      "outputs": [],
      "source": [
        "# lets have a look at the shapes of data\n",
        "print(\"X_TRAIN\",X_train.shape)\n",
        "print(\"Train_features\",X_train_feat.shape)\n",
        "print(\"X_VALID\",X_valid.shape)\n",
        "print(\"Valid_features\",X_valid_feat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3GjeCBDPIl3"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "  # we will create two inputs one which has sequence of just tokens and\n",
        "  # other which has feature of each token (so array of features of each token)\n",
        "  x1 = Input(shape=(max_len,))\n",
        "  x2 = Input(shape = (max_len,X_train_feat.shape[2],1))\n",
        "#----------------------------------------TOKEN LSTM -----------------------------------\n",
        "\n",
        "  mask1 = layers.Masking(mask_value=0)(x1)\n",
        "  embed = layers.Embedding(input_dim= len(tokenizer_.word_index), output_dim=128, input_length=max_len)(mask1)\n",
        "\n",
        "\n",
        "#---------------------------------------------FEATURE CONVOLUTION-----------------------------\n",
        "\n",
        "  mask2 = layers.Masking(mask_value=-1)(x2)\n",
        "  conv1 = layers.Conv2D(filters = 64, kernel_size= 4, strides = 1, padding= \"same\", activation = 'relu')(mask2)\n",
        "  conv2 = layers.Conv2D(filters = 32, kernel_size= 4, strides = 1, padding= \"same\", activation = 'relu')(conv1)\n",
        "  reshape1 = layers.Reshape((317,160), input_shape = (317,5,32))(conv2)\n",
        "  # you can add more layers and make network more complex here\n",
        "\n",
        "#----------------------------------------------Merging both---NOW BILSTM starts-------------------\n",
        "\n",
        "  merge = layers.concatenate([embed,reshape1])\n",
        "  drop1 = layers.Dropout(0.5)(merge)\n",
        "\n",
        "  lstm1 = layers.LSTM(units= 200, return_sequences= True, dropout= 0.4)(drop1)\n",
        "  bilstm1 = layers.Bidirectional(LSTM(units= 200, return_sequences= True, dropout = 0.2))(lstm1)\n",
        "  bilstm2 = layers.Bidirectional(LSTM(units= 100, return_sequences= True))(bilstm1)\n",
        "  norm1 = layers.BatchNormalization(axis = -1)(bilstm2)\n",
        "  # you can add more layers and make model more complex.\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "  # KEEP THIS INTACT\n",
        "  predictions = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(norm1)\n",
        "  model = Model(inputs = ([x1,x2]), outputs = predictions)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv80GZ6duryR"
      },
      "outputs": [],
      "source": [
        "# function to plot the loss, precision and recall, \n",
        "# caution code block has lines to save the files to drive.\n",
        "def display_history(history,name =\"None\"):\n",
        "\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import numpy as np                                        \n",
        "    # \"\"\"Summarize history for accuracy and loss.\n",
        "\n",
        "    sns.set_palette(\"pastel\")\n",
        "    sns.set(style=\"darkgrid\")\n",
        "    path = '/content/drive/MyDrive/..../'\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Loss_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['loss'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_loss'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('loss')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig(path + title + 'CONV2D.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Precision_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['precision'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_precision'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('precision')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig( path + title + 'CONV2D.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Recall_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['recall'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_recall'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('recall')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig( path + title + 'CONV2D.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByroETMzC5Xa"
      },
      "outputs": [],
      "source": [
        "# creating the F1 score function\n",
        "# in case you load the saved model and want to train further you will need to pass this in custom object \n",
        "# if you need to get F1 score.\n",
        "def f1_metric():\n",
        "  def F1(y_true, y_pred):\n",
        "    true_positives = be.sum(be.round(be.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = be.sum(be.round(be.clip(y_true, 0, 1)))\n",
        "    predicted_positives = be.sum(be.round(be.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + be.epsilon())\n",
        "    recall = true_positives / (possible_positives + be.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+be.epsilon())\n",
        "    \n",
        "    return f1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfMGG26-AXnx"
      },
      "outputs": [],
      "source": [
        "# this will take the tag = 'date'/'name'/'address'.....make the target encoding as per that and load and compile the model\n",
        "def LoadandCompile(tag):\n",
        "  be.clear_session()\n",
        "  temp = d_train[d_train.dataType == 'TRAIN']['{}Target'.format(tag)].tolist()\n",
        "  y_train = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "  y_train = np.array(y_train)\n",
        "\n",
        "  temp = data[data.dataType == 'VALIDATION']['{}Target'.format(tag)].tolist()\n",
        "  y_valid = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "  y_valid = np.array(y_valid)\n",
        "\n",
        "  # this is cutom entity so we cas use binary cross entropy.\n",
        "  loss = binary_crossentropy\n",
        "  optimizer=RectifiedAdam(learning_rate = 0.001, warmup_proportion = 0.2, beta_1 = 0.9, \n",
        "                                            total_steps= 2000, min_lr= 0.0008)\n",
        "  model = build_model()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss= loss,\n",
        "                metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(),tf.keras.metrics.Recall(), f1_metric])\n",
        "  \n",
        "\n",
        "  return model, y_train, y_valid  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdy13tKHjJwp"
      },
      "outputs": [],
      "source": [
        "# dataframe for bookkeeping\n",
        "historys = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3aMwnYCdXQAq",
        "outputId": "733b2f95-4b78-4911-9300-04b87847fd5a"
      },
      "outputs": [],
      "source": [
        "# lets run the model for each annotation\n",
        "annotations  = ['date', 'name', 'address', 'amount']\n",
        "\n",
        "for i in annotations:\n",
        "  be.clear_session()\n",
        "  model,y_train, y_valid = LoadandCompile(i)\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_metric',patience= 25,\n",
        "                                                   mode='max', restore_best_weights = True)\n",
        "\n",
        "  print(\"Starting training {} on with model presented above\".format(i))\n",
        "  history = model.fit(x=[X_train,X_train_feat], y=y_train,\n",
        "                    validation_data=([X_valid,X_valid_feat], y_valid),\n",
        "                    epochs=300,batch_size=32,  callbacks = [early_stopping])\n",
        "  \n",
        "  #   # saving the model\n",
        "  model.save('/content/drive/MyDrive/....../{}_conv2d_earlystopping.h5'.format(i))\n",
        "  val_loss, val_acc, val_prec, val_recall, val_F1 = model.evaluate([X_valid,X_valid_feat],y_valid)\n",
        "  historys.append([i, val_loss,val_acc,val_prec,val_recall,val_F1])\n",
        "  display_history(history, name = i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BvGmVrTzMrFY",
        "outputId": "15d58d70-7029-4b25-8f4d-c9d05a581331"
      },
      "outputs": [],
      "source": [
        "# as amount behaves very differently, the patience value is changed to 40 and batch size to 16.\n",
        "annotations  = ['amount']\n",
        "\n",
        "for i in annotations:\n",
        "  be.clear_session()\n",
        "  model,y_train, y_valid = LoadandCompile(i)\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_metric',patience= 40,\n",
        "                                                   mode='max', restore_best_weights = True)\n",
        "\n",
        "  print(\"Starting training {} on with model presented above\".format(i))\n",
        "  history = model.fit(x=[X_train,X_train_feat], y=y_train,\n",
        "                    validation_data=([X_valid,X_valid_feat], y_valid),\n",
        "                    epochs=300,batch_size=16,  callbacks = [early_stopping])\n",
        "  \n",
        "  # saving the model\n",
        "  model.save('/content/drive/MyDrive/............/{}_conv2d_earlystopping.h5'.format(i))\n",
        "  val_loss, val_acc, val_prec, val_recall, val_F1 = model.evaluate([X_valid,X_valid_feat],y_valid)\n",
        "  historys.append([i, val_loss,val_acc,val_prec,val_recall,val_F1])\n",
        "  display_history(history, name = i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PBgxa7nbT63"
      },
      "outputs": [],
      "source": [
        "# we have got the precision and recall during the training but that is at the token level, \n",
        "# we need now to see the metrics at invoice level as thats the main problem we are solving.\n",
        "\n",
        "def invoiceMetrices(y_pred, tag, df, threshold):\n",
        "  \n",
        "  df = df.reset_index()\n",
        "  y_pred  = (y_pred> threshold).astype(int)\n",
        "  predicted ={}\n",
        "  predicted['{}'.format(tag)] = y_pred\n",
        "  predicted_df = {'{}'.format(tag):['null']}\n",
        "\n",
        "  for a,b in zip(df.sentTokens, predicted['{}'.format(tag)]):\n",
        "      b = b.flatten()\n",
        "      temp = [item for item in b[:len(a)]]\n",
        "      predicted_df['{}'.format(tag)].append(temp)\n",
        "  predicted_df['{}'.format(tag)] = predicted_df['{}'.format(tag)][1:]\n",
        "  predicted_df = pd.DataFrame.from_dict(predicted_df)\n",
        "  df['predicted_{}_indexs'.format(tag)] = predicted_df['{}'.format(tag)]\n",
        "\n",
        "  TP = 0\n",
        "  for a,b in zip(df['{}Target'.format(tag)],df['predicted_{}_indexs'.format(tag)]):\n",
        "    count = (a == b)\n",
        "    if type(count) == type(True):\n",
        "      TP += 1\n",
        "    elif sum(count) == len(a):\n",
        "      TP += 1\n",
        "  return (TP/len(y_pred))*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHlZ7eU8uHqJ",
        "outputId": "3a2d8886-1659-4702-bfa5-97030c217838"
      },
      "outputs": [],
      "source": [
        "# book keeping\n",
        "report = {'Label':[], 'Accuracy':[], 'Threshold': []}\n",
        "# making testdata model ready\n",
        "X_test_feat = np.expand_dims(X_test_feat,-1)\n",
        "print(X_test.shape)\n",
        "print(X_test_feat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIJ66mvWmPCd"
      },
      "outputs": [],
      "source": [
        "# getting the final metrics for each annotation\n",
        "annotations  = ['amount','date','name','address']\n",
        "threshold = [0.3, 0.35, 0.4,0.45, 0.5, 0.55, 0.6, 0.65]\n",
        "for confidencescore in threshold:\n",
        "\n",
        "  for tag in annotations:\n",
        "    be.clear_session()\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/...../Conv2D/{}_conv2d_earlystopping.h5'.format(tag), compile = False)   \n",
        "    # custom_objects={ 'metrics': f1_metric(y_valid, ) })\n",
        "    temp = data[data.dataType == 'TEST']['{}Target'.format(tag)].tolist()\n",
        "    y_test = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "    y_test = np.array(y_test)\n",
        "    y_pred = model.predict([X_test,X_test_feat])\n",
        "    acc = invoiceMetrices(y_pred,tag,data[data.dataType == 'TEST'],confidencescore)\n",
        "    report['Label'].append(tag)\n",
        "    report['Accuracy'].append(acc)\n",
        "    report['Threshold'].append(confidencescore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WD3O5T_jxL6V",
        "outputId": "dd1aebda-96cf-4e09-cf1e-752d1cd4ee50"
      },
      "outputs": [],
      "source": [
        "report = pd.DataFrame.from_dict(report)\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TlfdRWwaj4Z"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "path = '/content/drive/MyDrive/.../'  # path to the folder in google drive where data is saved\n",
        "with open(path + \"X_train_feat.ob\", 'wb') as a, open(path + \"X_valid_feat.ob\", 'wb') as b, open(path + \"X_test_feat.ob\", 'wb') as c:\n",
        "    pickle.dump(X_train_feat, a) \n",
        "    pickle.dump(X_valid_feat, b)\n",
        "    pickle.dump(X_test_feat, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvdTFLBj3Z1g"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/.../'  # path to the folder in google drive where data is saved\n",
        "with open(path + \"posdict.ob\", 'wb') as a, open(path + \"shapedict.ob\", 'wb') as b, open(path + \"data.ob\", 'wb') as c:\n",
        "    pickle.dump(posdict, a) \n",
        "    pickle.dump(shapedict, b)\n",
        "    pickle.dump(data, c)\n",
        "\n",
        "with open(path + \"tokenizer.ob\", 'wb') as a, open(path + \"Conv2Dresults.ob\", 'wb') as b:\n",
        "  pickle.dump(tokenizer_, a)\n",
        "  pickle.dump(history, b)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "token_Conv2d.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3472910ebd2aa62b6c54b483bd66ffdd88459b69fe7bc051f28b07b9e0034dc5"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('tf_keras': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
