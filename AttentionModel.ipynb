{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muampvt-cTo7"
      },
      "source": [
        "This notebook is part of 4 approaches. This notebook implements the Entity Extarction by exploiting the coordiantes of token and using the 'Attention' model. The final assessment is done by comparing the each token labels and if it 'exactly' matches the actual label then only the prediction is considered correct ( this was done to benchmark it with Google AUTO ML part ). \n",
        "\n",
        "This model is trained on all labels in one go, therefore like previous approaches it has one model only which predicts all the 4 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCC9hrMEZLJ1",
        "outputId": "44654abb-2e81-4f8d-cd1f-56f0ba9ba002"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OvYRjD9PxZj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import operator\n",
        "from tensorflow.keras import backend as be\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.constraints import maxnorm\n",
        "from tensorflow_addons.optimizers import RectifiedAdam\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.losses import categorical_crossentropy, sparse_categorical_crossentropy, binary_crossentropy\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,Adadelta,RMSprop\n",
        "from tensorflow.keras  import losses\n",
        "from tensorflow.keras import backend as be\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from numpy.random import seed\n",
        "seed(7)\n",
        "tf.compat.v1.set_random_seed(7)\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqbh4OCqP8DS",
        "outputId": "8399eab0-942b-4e6e-b9b1-24dae8cc6804"
      },
      "outputs": [],
      "source": [
        "# Lets mount the Google Drive and acccess the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHrV3fSBPxZl"
      },
      "source": [
        "The code for reading and extracting the information from jsonl file.\n",
        "\n",
        "##### EXTRACTION OF LABELS START OFFSET AND END OFFSET VALUES and TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCYPj5LnPxZm",
        "outputId": "b02a724f-29b2-4c1c-c3d8-3c6653bf1d11"
      },
      "outputs": [],
      "source": [
        "# as the the file type is jsonl, each invoice is represented by hierarchial json object, therefore\n",
        "# we need to fetch each invoice in hierarchial manner\n",
        "# further this will access the folder from where to read the fields, so please change the path \n",
        "# as per your specifications.\n",
        "\n",
        "documents = []\n",
        "\n",
        "# each invoice belogs to either train, validation, test set\n",
        "category = []\n",
        "\n",
        "# with the json object of each invoice we also have another json object with key (label), value (string) pair\n",
        "# each annotation(label) has been given one initial 'null' value as there cannot be empty dictionary with keys.\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "\n",
        "# this is similar to placeholder but will read the tokens provided by OCR for each invoice.\n",
        "tokens = {'sentTokens':['null'], 'sentToken_startOffset':['null'], 'sentToken_endOffset':['null'],'boundingPoly':['null']}\n",
        "\n",
        "\n",
        "# this reads the csvfile which gives list of all json files\n",
        "list_ = pd.read_csv('/content/drive/MyDrive/......csv',usecols=[0,2], names=['Type', 'Filename'], header=None)\n",
        "for type_, file_name in list_.itertuples(index=False):\n",
        "\n",
        "    # read the jsonfile\n",
        "    with open('/content/drive/MyDrive/..../{}'.format(file_name), 'r', encoding=\"utf8\") as json_file:\n",
        "        json_list = list(json_file)\n",
        "\n",
        "    # each invoice is encapsulated in one dictionary with many key-value pairs\n",
        "    for json_str in json_list:\n",
        "        result = json.loads(json_str)\n",
        "\n",
        "\n",
        "        #-----------------READING THE RAW TEXT AND INVOICE TYPE-------------------------------------------\n",
        "        # extracting the raw text provided by OCR \n",
        "        documents.append(result['document']['documentText']['content'])\n",
        "\n",
        "        # extracting the invoice type (train/validation/test) \n",
        "        category.append(type_)\n",
        "\n",
        "        #------------------EXTRACTING OFFSET VALUES FOR LABELS------------------------------------------\n",
        "        # As not all invoices has all the 4 annotations we need to keep track of same\n",
        "        truth_table = {'Date': False, 'Address': False, 'Name': False, 'Amount':False}\n",
        "\n",
        "        # extracting the annotations and appending them to the placeholder\n",
        "        for i in range(len(result['annotations'])):\n",
        "                placeholder[result['annotations'][i]['displayName']].append(result['annotations'][i]['textExtraction']['textSegment'])\n",
        "                truth_table[result['annotations'][i]['displayName']] = True\n",
        "        \n",
        "        # if any annotation not available for invoice them append empty value for same \n",
        "        for key, value in truth_table.items():\n",
        "            if value == False:\n",
        "                placeholder[key].append({})\n",
        "\n",
        "        #--------------------------------EXTRACTING TOKENS GIVEN BY OCR for EACH INVOICE and its bounding Polygon--------------------------\n",
        "        temp = {'tokensOfinvoice':[],'polygons':[]}\n",
        "        start_off = []\n",
        "        end_off = []\n",
        "\n",
        "        # extracting the raw text provided by OCR \n",
        "        for i in range(len(result['document']['layout'])):\n",
        "            temp['tokensOfinvoice'].append(result['document']['layout'][i]['textSegment']['content'])\n",
        "            temp['polygons'].append(result['document']['layout'][i]['boundingPoly']['normalizedVertices'])\n",
        "            if 'startOffset' in result['document']['layout'][i]['textSegment']:\n",
        "                start_off.append(int(result['document']['layout'][i]['textSegment']['startOffset']))\n",
        "            else:\n",
        "                start_off.append(0)\n",
        "            \n",
        "            if 'endOffset' in result['document']['layout'][i]['textSegment']:\n",
        "                end_off.append(int(result['document']['layout'][i]['textSegment']['endOffset']))\n",
        "            else: \n",
        "                end_off.append(-1)\n",
        "\n",
        "        tokens['sentTokens'].append(temp['tokensOfinvoice'])\n",
        "        tokens['boundingPoly'].append(temp['polygons'])\n",
        "        tokens['sentToken_startOffset'].append(start_off)\n",
        "        tokens['sentToken_endOffset'].append(end_off)\n",
        "\n",
        "\n",
        "# getting the list of all annotations offset and tokens ( removing first null entry )\n",
        "placeholder['Date'] = placeholder['Date'][1:]\n",
        "placeholder['Address']= placeholder['Address'][1:]\n",
        "placeholder['Name'] = placeholder['Name'][1:]\n",
        "placeholder['Amount'] = placeholder['Amount'][1:]\n",
        "\n",
        "placeholder['sentTokens'] = tokens['sentTokens'][1:]\n",
        "placeholder['sentToken_startOffset'] = tokens['sentToken_startOffset'][1:]\n",
        "placeholder['sentToken_endOffset'] = tokens['sentToken_endOffset'][1:]\n",
        "placeholder['boundingPoly'] = tokens['boundingPoly'][1:]\n",
        "# appending the raw text and type of invoice\n",
        "placeholder['text'] = documents\n",
        "placeholder['dataType'] = category\n",
        "\n",
        "\n",
        "# checking for count of all annotations and documents\n",
        "print(placeholder.keys())\n",
        "print(len(placeholder['Amount']))\n",
        "print(len(placeholder['Date']))\n",
        "print(len(placeholder['Address']))\n",
        "print(len(placeholder['Name']))\n",
        "print(len(placeholder['text']))\n",
        "print(len(placeholder['dataType']))\n",
        "\n",
        "data = pd.DataFrame.from_dict(placeholder)\n",
        "\n",
        "# Remember we have dictionary containing the start and end offset, we can extract them \n",
        "# separately too, if required.\n",
        "#--------------------------------------------------------------------------\n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']:\n",
        "    # sometimes startoffset might not be present if annotation start from first index of raw text\n",
        "    data['{}_start'.format(keys)] = data[keys].apply(lambda x: int(x['startOffset']) if 'startOffset' in x.keys()\n",
        "                        else ( 0 if 'endOffset' in x.keys() else None ))\n",
        "\n",
        "    # similarly as above endoffset might not be present if the annotations end is last index of raw text                 \n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']:   \n",
        "    data['{}_end'.format(keys)] = data[keys].apply(lambda x: int(x['endOffset']) if 'endOffset' in x.keys()\n",
        "                        else ( -1 if 'startOffset' in x.keys() else None))\n",
        "\n",
        "data = data.drop(['Date', 'Address', 'Name', 'Amount'], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n1Jcq7nPxZr"
      },
      "source": [
        "Lets contruct the target for the models, the OCR gives the startoffset and endoffset for each label, however this is at character. To make the encoding for each label at otken level, we will use simple logic if the startOffset of token lies within the bound of actual label then its part of label else not. This approach has limitation, because if the actual label is sub-part of token then this will exclude that token.This is one limitation of using the output of OCR. This same cause is also the reason for the AutoML bad performance, because any model will work on token level, however if the labelling is done by taking the sub-span within token then model is not able to take that into factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UhY6-bmPxZs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# placeholder to keep the target of each label\n",
        "import math\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "for i,j in placeholder.items():\n",
        "    for a,b,c,d in zip(data['sentToken_startOffset'],data['sentToken_endOffset'],data['{}_start'.format(i)], data['{}_end'.format(i)]):\n",
        "\n",
        "        if not math.isnan(c):\n",
        "\n",
        "            start_left = np.array(a) >=c\n",
        "            start_right = np.array(a) <=d\n",
        "            start_truth_table = start_left == start_right\n",
        "            \n",
        "            end_left = np.array(b)  >= c\n",
        "            end_right = np.array(b) <=d\n",
        "            end_truth_table = end_left == end_right\n",
        "            truth_table = start_truth_table + end_truth_table \n",
        "\n",
        "            placeholder['{}'.format(i)].append(truth_table.astype(int))\n",
        "        else:\n",
        "            placeholder['{}'.format(i)].append(list(np.zeros(len(a)).astype(int)))\n",
        "\n",
        "# removing the first null entry.\n",
        "placeholder['dateTarget'] = placeholder['Date'][1:]\n",
        "placeholder['addressTarget']= placeholder['Address'][1:]\n",
        "placeholder['nameTarget'] = placeholder['Name'][1:]\n",
        "placeholder['amountTarget'] = placeholder['Amount'][1:]\n",
        "\n",
        "del placeholder['Address'], placeholder['Amount'], placeholder['Date'], placeholder['Name']\n",
        "\n",
        "# creating final dataframe \n",
        "y = pd.DataFrame.from_dict(placeholder)\n",
        "data = pd.concat([data,y], axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGGTl7B_PxZs",
        "outputId": "a994024d-63cf-4c6c-b729-e9af2eb04395"
      },
      "outputs": [],
      "source": [
        "# If any token is present in label, then the the max value in the list for that sentence for that label will be 1, \n",
        "# hence if eveything is fine then we shoud see same numbers for all labels. However as highlighted due to slicing of token in case of amount, \n",
        "# it is have mis match. \n",
        "# Ex: if the token is RM170.00, now in labelling the actual value of amount is just 170.00, but since this value is sub-span of this whole token, \n",
        "# the encoding cannnot be done eeasily, therefore we to take care of such cases we take the whole token rather than just the sub-span.\n",
        "\n",
        "print('number of Date labels', sum([max(x) for x in data.dateTarget]))\n",
        "print('number of Name labels', sum([max(x) for x in data.nameTarget]))\n",
        "print('number of Address labels', sum([max(x) for x in data.addressTarget]))\n",
        "print('number of Amount labels', sum([max(x) for x in data.amountTarget]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br4q7TA2PxZt"
      },
      "source": [
        "Before we proceed lets invistigate if everything is okay in terms of data frame creation or not. We will do this by extracting the token from annotations encoding for each label and comparing with the actuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MqX4ZcC0Eo9"
      },
      "outputs": [],
      "source": [
        "## get the tokens set which form part of each label.\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.dateTarget) ]\n",
        "data['dateTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.nameTarget) ]\n",
        "data['nameTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.addressTarget) ]\n",
        "data['addressTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.amountTarget) ]\n",
        "data['amountTokens'] = temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6emR4RyqPxZu",
        "outputId": "d4c3522e-3f69-41d6-f636-cfde2760efaf"
      },
      "outputs": [],
      "source": [
        "# lets check the whole dataframe\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fxzhdf00NPR"
      },
      "outputs": [],
      "source": [
        "## Lets read the actual text which form part of each label\n",
        "# placeholder for annotations text\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "\n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']: \n",
        "    # read annotation_start, annotation_end and text from data\n",
        "    for a,b,c in data[['{}_start'.format(keys), '{}_end'.format(keys), 'text']].itertuples(index=False):\n",
        "        # extracting the text\n",
        "        if not np.isnan(a) and  not np.isnan(b):\n",
        "            placeholder[keys].append(c[int(a):int(b)])\n",
        "        # if annotation not present then append None\n",
        "        else:\n",
        "            placeholder[keys].append(None)\n",
        "\n",
        "placeholder['dateTextactual'] = placeholder['Date'][1:]\n",
        "placeholder['addressTextactual'] = placeholder['Address'][1:]\n",
        "placeholder['nameTextactual'] = placeholder['Name'][1:]\n",
        "placeholder['amountTextactual'] = placeholder['Amount'][1:]\n",
        "\n",
        "del placeholder['Address'], placeholder['Amount'], placeholder['Date'], placeholder['Name']\n",
        "\n",
        "y = pd.DataFrame.from_dict(placeholder)\n",
        "\n",
        "###-------Lets construct the each annotations from tokens and targetencoding\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.dateTokens]\n",
        "y['dateFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.nameTokens]\n",
        "y['nameFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.addressTokens]\n",
        "y['addressFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.amountTokens]\n",
        "y['amountFromTokens'] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "oE28fCVC0QZa",
        "outputId": "57b356bb-ddb3-4738-c854-57a2d3264155"
      },
      "outputs": [],
      "source": [
        "# just for demo..purpose\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GJfsAFJmHXc"
      },
      "source": [
        "Lets start building the model now. We will start with feature extraction. For this we will use the spacy to give features values for each token. \n",
        "\n",
        "> \n",
        "*   Feature1: index of token in the invoice\n",
        "*   Feature2: Is token alphanumeric or not\n",
        "*   Feature3: Whether Token is like Num or not ( its different from is-alpha Ex: 01-11189 will have like_Num =  True and is_alpha = False, but the token 21:15 will have like_Num = False and is_alpha = False, as the other is likely representing time and is not purely a Number)\n",
        "*   Feature4: length of token\n",
        "Lets have a look what kind of values we get one invoice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVuoqfKATMag"
      },
      "outputs": [],
      "source": [
        "##lets explore what kind of values we get, and what it means by these features\n",
        "\n",
        "# function to take the list of tokens for each invoice and return the features for each token\n",
        "\n",
        "def invoiceFeatures(sent,polygons): \n",
        "    idx = 0\n",
        "    temp1 = []\n",
        "    temp2 = []\n",
        "    for word, box in zip(sent, polygons):\n",
        "        box[0]\n",
        "        # as some token dont have x or y coordinate if it start from 0, therefore need to check for that\n",
        "        indices = [ checkMissingCoordinate(box[0]), checkMissingCoordinate(box[1]), checkMissingCoordinate(box[2]), checkMissingCoordinate(box[3])]\n",
        "        temp1.append(indices)\n",
        "        feat = [idx, word.isalpha(), word.isalnum(), len(word)]\n",
        "        temp2.append(feat)\n",
        "        idx +=1\n",
        "    temp1, temp2  = np.array(temp1), np.array(temp2)\n",
        "    return np.concatenate((temp1.reshape(temp1.shape[0],8) , temp2), axis = 1)\n",
        "\n",
        "# function to iterate over the all the invoice, this will call sent2features internally\n",
        "def prep_features(sents,polygonsList):\n",
        "    sent_features = []\n",
        "    \n",
        "    for sent,polygons in zip(sents, polygonsList):\n",
        "        temp = invoiceFeatures(sent, polygons)\n",
        "        temp = np.transpose(temp)\n",
        "        temp = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len, value = -1, dtype= 'float32')\n",
        "        temp = np.transpose(temp)\n",
        "        # print(temp)\n",
        "        sent_features.append(temp)\n",
        "    return np.array(sent_features)\n",
        "\n",
        "def checkMissingCoordinate(coordinatePair):\n",
        "    temp = []\n",
        "\n",
        "    if 'x' in coordinatePair.keys():\n",
        "        temp.append(coordinatePair['x'])\n",
        "    else:\n",
        "        temp.append(0.0)\n",
        "    \n",
        "    if 'y' in coordinatePair.keys():\n",
        "        temp.append(coordinatePair['y'])\n",
        "    else: \n",
        "        temp.append(0.0)\n",
        "    \n",
        "    return temp\n",
        "\n",
        "  \n",
        "# this is just some tokenizing technique using keras and create the library of tokens...\n",
        "# lets build custom tokenizer adapted for this corpus\n",
        "\n",
        "def tokenize(lang, num_words = None):\n",
        "    # lang = list of sentences in a language\n",
        "\n",
        "    # lets select the default config and basic Tokenizer from keras\n",
        "    \n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', num_words= num_words)\n",
        "\n",
        "    # this step is necessary to allow tokenzier build its words id for the corpus \n",
        "    # from the lang = list of sentences\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "    ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "    ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "    ## and pads the sequences to match the longest sequences in the given input\n",
        "    # this will make all sentences of same length\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', value = 0)\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPCU2xthUPsW",
        "outputId": "8f053819-d9f8-48eb-e77c-143b709d9095"
      },
      "outputs": [],
      "source": [
        "\n",
        "# getting max length of sequence of tokens for invoices\n",
        "# we need this variable for padding\n",
        "\n",
        "max_len = max([len(x) for x in data.sentTokens])\n",
        "print(\"max length is {}\".format(max_len))\n",
        "\n",
        "# we will be dropping the data where any label is missing, this will be done in training and validation\n",
        "type_data = ['TRAIN', 'VALIDATION', 'TEST']\n",
        "d_train = data[data.dataType == 'TRAIN']\n",
        "d_valid =  data[data.dataType == 'VALIDATION']\n",
        "\n",
        "d_train = d_train.dropna(axis = 0, subset= ['Date_start','Name_start','Address_start','Amount_start','Date_end','Address_end','Name_end','Amount_end'])\n",
        "d_valid = d_valid.dropna(axis = 0, subset= ['Date_start','Name_start','Address_start','Amount_start','Date_end','Address_end','Name_end','Amount_end'])\n",
        "\n",
        "X_train_feat = prep_features(d_train.sentTokens, d_train.boundingPoly)\n",
        "X_valid_feat = prep_features(d_valid.sentTokens, d_valid.boundingPoly)\n",
        "X_test_feat = prep_features(data[data.dataType == 'TEST'].sentTokens, data[data.dataType == 'TEST'].boundingPoly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJwaB7Poc2Z8"
      },
      "outputs": [],
      "source": [
        "# getting token ids for each token and padding each invoice\n",
        "# Further we split the data (train, validation and test)\n",
        "# splitting the data\n",
        "\n",
        "X_train, tokenizer_ = tokenize(d_train.sentTokens)\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len, padding='post', value = 0)\n",
        "\n",
        "X_valid = tokenizer_.texts_to_sequences(d_valid.sentTokens)\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=max_len, padding='post', value = 0)\n",
        "\n",
        "X_test = tokenizer_.texts_to_sequences(data[data.dataType == 'TEST'].sentTokens)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len, padding='post', value = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEeHWtoxh5lv",
        "outputId": "43d7793c-7e1b-421e-d271-bbe5814b572e"
      },
      "outputs": [],
      "source": [
        "# lets have a look at the shapes of data\n",
        "\n",
        "print(\"X_TRAIN\",X_train.shape)\n",
        "print(\"Train_features\",X_train_feat.shape)\n",
        "print(\"X_VALID\",X_valid.shape)\n",
        "print(\"Valid_features\",X_valid_feat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQzH8cYy4SkA"
      },
      "outputs": [],
      "source": [
        "# this is required as to tell the input shape to model.\n",
        "shape_ = X_train_feat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpKUOyItzKZc"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "  # we will create two inputs one which has sequence of just tokens and\n",
        "  # other which has feature of each token (so array of features of each token)\n",
        "  x1 = Input(shape=(max_len,))\n",
        "  x2 = Input(shape = (max_len,X_train_feat.shape[2]))\n",
        "#----------------------------------------TOKEN Embedding -----------------------------------\n",
        "\n",
        "  mask1 = layers.Masking(mask_value=0)(x1)\n",
        "  embed = layers.Embedding(input_dim= len(tokenizer_.word_index), output_dim=128, input_length=max_len)(mask1)\n",
        "  norm1 = layers.BatchNormalization(axis=1)(embed)\n",
        "\n",
        "#---------------------------------------------FEATURE-----------------------------\n",
        "\n",
        "  mask2 = layers.Masking(mask_value=-1)(x2)\n",
        "#----------------------------------------------Merging both---NOW first we pass through memory(LSTM) and Dense(for more complex fetaure)......\n",
        "\n",
        "  merge = layers.concatenate([norm1,mask2])\n",
        "  drop1 = layers.Dropout(0.2)(merge)\n",
        "  lstm1 = layers.LSTM(units= 300, return_sequences= True)(drop1)\n",
        "  denseLinear = layers.Dense(units= 300, activation = 'relu')(lstm1)\n",
        "# ------------------------ and now attention starts ( attention acts on memory (LSTM)-----------------------\n",
        "  attention = layers.Dense(1, activation='tanh')(lstm1)\n",
        "  attention = layers.Flatten()(attention)\n",
        "  attention = layers.Activation('softmax')(attention)\n",
        "  attention = layers.RepeatVector(300)(attention)\n",
        "  attention = layers.Permute([2, 1])(attention)\n",
        "# -------------------------------------------------Attention tells LSTM what to remember and what to forget---------------\n",
        "  sent_representation = layers.Multiply()([lstm1, attention])\n",
        "\n",
        "  dense = layers.Dense(512, activation = 'relu')(sent_representation)\n",
        "  drop1 = layers.Dropout(0.3)(dense)\n",
        "  dense2 = layers.Dense(128)(drop1)\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "#   # KEEP THIS INTACT\n",
        "  predictions = layers.Dense(5, activation='softmax')(dense2)\n",
        "\n",
        "  model = Model(inputs = ([x1,x2]), outputs = predictions)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI0b4IrVSfFq"
      },
      "outputs": [],
      "source": [
        "# since now we ahve all annotations together we create target encoding for same\n",
        "\n",
        "def createTargetForMultilabel():\n",
        "   # with this now 0: No label, 1: name, 2: address, 3: date, 4: amount\n",
        "  d_train['tags'] = d_train.nameTarget + d_train.addressTarget*2 + d_train.dateTarget*3 + d_train.amountTarget*4 \n",
        "  a = d_train.tags.tolist()\n",
        "  y_train = tf.keras.preprocessing.sequence.pad_sequences(a, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "\n",
        "  d_valid['tags'] = d_valid.nameTarget + d_valid.addressTarget*2 + d_valid.dateTarget*3 +  d_valid.amountTarget*4\n",
        "  a = d_valid.tags.tolist()\n",
        "  y_valid = tf.keras.preprocessing.sequence.pad_sequences(a, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "\n",
        "# one hot encode target\n",
        "  y_train = [tf.keras.utils.to_categorical(i, num_classes=5) for i in y_train]\n",
        "  y_valid = [tf.keras.utils.to_categorical(i, num_classes=5) for i in y_valid]\n",
        "\n",
        "  y_train = np.array(y_train)\n",
        "  y_valid = np.array(y_valid)\n",
        "  return y_train, y_valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiAuDs9B1ua7"
      },
      "outputs": [],
      "source": [
        "# function to plot the loss, precision and recall, \n",
        "# caution code block has lines to save the files to drive.\n",
        "\n",
        "def display_history(history,name =\"None\"):\n",
        "\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import numpy as np                                        \n",
        "    # notebook and use it by doing slight modification\n",
        "    # \"\"\"Summarize history for accuracy and loss.\n",
        "\n",
        "    sns.set_palette(\"pastel\")\n",
        "    sns.set(style=\"darkgrid\")\n",
        "    path = '/content/drive/MyDrive/..../'\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Loss_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['loss'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_loss'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('loss')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig(path + title + '.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Precision_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['precision'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_precision'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('precision')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig( path + title + '.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Recall_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['recall'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_recall'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('recall')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig( path + title + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw8oLIBaZXF7"
      },
      "outputs": [],
      "source": [
        "# 0: None, 1: name, 2: address, 3: date, 4: amount\n",
        "def weighted_loss(class_weights):\n",
        "    weights = list(class_weights.values())\n",
        "    def loss(y_true, y_pred):\n",
        "\n",
        "        y_pred = be.clip(y_pred, be.epsilon(), 1 - be.epsilon())\n",
        "        \n",
        "        loss = y_true * be.log(y_pred) * weights\n",
        "        loss = -be.sum(loss, axis= -1)\n",
        "        return loss\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZYQAIWkRuyT"
      },
      "outputs": [],
      "source": [
        "# this will take the tag = 'date'/'name'...make the target encoding as per that and load and compile the model\n",
        "def LoadandCompile(tag):\n",
        "  be.clear_session()\n",
        "\n",
        "  y_train, y_valid =  createTargetForMultilabel()\n",
        "  class_weights = {0:0.05, 1:100, 2:20, 3: 200, 4: 180}\n",
        "  loss = weighted_loss(class_weights = class_weights)\n",
        "\n",
        "  optimizer=RectifiedAdam(learning_rate = 0.001, warmup_proportion = 0.3, beta_1 = 0.9,weight_decay= 0.05,   \n",
        "                                            total_steps= 2500, min_lr= 0.0001)\n",
        "  model = build_model()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss= loss,\n",
        "                metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
        "  \n",
        "  return model, y_train, y_valid  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aMwnYCdXQAq"
      },
      "outputs": [],
      "source": [
        "# lets run the model for each annotation\n",
        "\n",
        "be.clear_session()\n",
        "\n",
        "model,y_train, y_valid = LoadandCompile('combined')\n",
        "\n",
        "print(\"Starting training {} on with model presented above\".format('combined'))\n",
        "history = model.fit(x=[X_train,X_train_feat], y=y_train,\n",
        "                  validation_data=([X_valid,X_valid_feat], y_valid),\n",
        "                  epochs=1200,batch_size=32) \n",
        "\n",
        "display_history(history, name = 'combined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ebIypruIJNb",
        "outputId": "1c90e44c-79d8-40d3-bce0-f4dbd0d6dad9"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/ING/{}_combined.h5'.format('attention'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL6GUJEFa-eS"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/ING/Attention/final/attention_combined_v3.h5', compile = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mkoBzgDmAi5",
        "outputId": "4abd6c17-6d8c-4fc1-bf52-f0742bbb1fa8"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z86QzPW3jBp0"
      },
      "outputs": [],
      "source": [
        "# lets do the prediction on test now and see the metrics\n",
        "a = model.predict([X_test,X_test_feat])\n",
        "\n",
        "b = a.tolist()\n",
        "df = data[data.dataType == 'TEST'].reset_index()\n",
        "df['probs'] = b\n",
        "def f(x,y,temp):\n",
        "  return(y[:len(x),temp])\n",
        "\n",
        "df['predictions'] = df.probs.apply(lambda x: (np.array(x) == np.array(x).max(axis =1)[:, None]).astype(int))\n",
        "df['predictedname'] = df.apply(lambda x: f(x.sentTokens,x.predictions,1), axis = 1)\n",
        "df['predictedaddress'] = df.apply(lambda x: f(x.sentTokens,x.predictions,2), axis = 1)\n",
        "df['predicteddate'] = df.apply(lambda x: f(x.sentTokens,x.predictions,3), axis = 1)\n",
        "df['predictedamount'] = df.apply(lambda x: f(x.sentTokens,x.predictions,4), axis = 1)\n",
        "\n",
        "\n",
        "#metric evlauation function\n",
        "def onehotinvoicesmetric(tag):\n",
        "  TP = 0\n",
        "  for a,b in zip(df['{}Target'.format(tag)],df['predicted{}'.format(tag)]):\n",
        "    count = (a == b)\n",
        "    if type(count) == type(True):\n",
        "      TP += 1\n",
        "    elif sum(count) == len(a):\n",
        "      TP += 1\n",
        "  return (TP/len(df))*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXGi1zVDXgLv",
        "outputId": "a8188257-eaff-496e-eca4-53cfb3456475"
      },
      "outputs": [],
      "source": [
        "print('DATE',onehotinvoicesmetric('date'))\n",
        "print('AMOUNT', onehotinvoicesmetric('amount'))\n",
        "print('NAME', onehotinvoicesmetric('name'))\n",
        "print('ADDRESS',onehotinvoicesmetric('address'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AttentionModel.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3472910ebd2aa62b6c54b483bd66ffdd88459b69fe7bc051f28b07b9e0034dc5"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('tf_keras': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
