{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muampvt-cTo7"
      },
      "source": [
        "This notebook is part of 4 approaches. This notebook implements the Entity Extarction by exploiting the coordiantes of token and using the 'Attention' model. The final assessment is done by comparing the each token labels and if it 'exactly' matches the actual label then only the prediction is considered correct ( this was done to benchmark it with Google AUTO ML part ). \n",
        "\n",
        "This model is trained on all labels in one go, therefore like previous approaches it has one model only which predicts all the 4 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCC9hrMEZLJ1",
        "outputId": "44654abb-2e81-4f8d-cd1f-56f0ba9ba002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OvYRjD9PxZj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import operator\n",
        "from tensorflow.keras import backend as be\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LSTM, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.constraints import maxnorm\n",
        "from tensorflow_addons.optimizers import RectifiedAdam\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.losses import categorical_crossentropy, sparse_categorical_crossentropy, binary_crossentropy\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam,Adadelta,RMSprop\n",
        "from tensorflow.keras  import losses\n",
        "from tensorflow.keras import backend as be\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from numpy.random import seed\n",
        "seed(7)\n",
        "tf.compat.v1.set_random_seed(7)\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqbh4OCqP8DS",
        "outputId": "8399eab0-942b-4e6e-b9b1-24dae8cc6804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Lets mount the Google Drive and acccess the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHrV3fSBPxZl"
      },
      "source": [
        "The code for reading and extracting the information from jsonl file.\n",
        "\n",
        "##### EXTRACTION OF LABELS START OFFSET AND END OFFSET VALUES and TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCYPj5LnPxZm",
        "outputId": "b02a724f-29b2-4c1c-c3d8-3c6653bf1d11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['Date', 'Address', 'Name', 'Amount', 'sentTokens', 'sentToken_startOffset', 'sentToken_endOffset', 'boundingPoly', 'text', 'dataType'])\n",
            "949\n",
            "949\n",
            "949\n",
            "949\n",
            "949\n",
            "949\n"
          ]
        }
      ],
      "source": [
        "# as the the file type is jsonl, each invoice is represented by hierarchial json object, therefore\n",
        "# we need to fetch each invoice in hierarchial manner\n",
        "# further this will access the folder from where to read the fields, so please change the path \n",
        "# as per your specifications.\n",
        "\n",
        "documents = []\n",
        "\n",
        "# each invoice belogs to either train, validation, test set\n",
        "category = []\n",
        "\n",
        "# with the json object of each invoice we also have another json object with key (label), value (string) pair\n",
        "# each annotation(label) has been given one initial 'null' value as there cannot be empty dictionary with keys.\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "\n",
        "# this is similar to placeholder but will read the tokens provided by OCR for each invoice.\n",
        "tokens = {'sentTokens':['null'], 'sentToken_startOffset':['null'], 'sentToken_endOffset':['null'],'boundingPoly':['null']}\n",
        "\n",
        "\n",
        "# this reads the csvfile which gives list of all json files\n",
        "list_ = pd.read_csv('/content/drive/MyDrive/ING/Filtered_data/text_extraction.csv',usecols=[0,2], names=['Type', 'Filename'], header=None)\n",
        "for type_, file_name in list_.itertuples(index=False):\n",
        "\n",
        "    # read the jsonfile\n",
        "    with open('/content/drive/MyDrive/ING/Filtered_data/{}'.format(file_name), 'r', encoding=\"utf8\") as json_file:\n",
        "        json_list = list(json_file)\n",
        "\n",
        "    # each invoice is encapsulated in one dictionary with many key-value pairs\n",
        "    for json_str in json_list:\n",
        "        result = json.loads(json_str)\n",
        "\n",
        "\n",
        "        #-----------------READING THE RAW TEXT AND INVOICE TYPE-------------------------------------------\n",
        "        # extracting the raw text provided by OCR \n",
        "        documents.append(result['document']['documentText']['content'])\n",
        "\n",
        "        # extracting the invoice type (train/validation/test) \n",
        "        category.append(type_)\n",
        "\n",
        "        #------------------EXTRACTING OFFSET VALUES FOR LABELS------------------------------------------\n",
        "        # As not all invoices has all the 4 annotations we need to keep track of same\n",
        "        truth_table = {'Date': False, 'Address': False, 'Name': False, 'Amount':False}\n",
        "\n",
        "        # extracting the annotations and appending them to the placeholder\n",
        "        for i in range(len(result['annotations'])):\n",
        "                placeholder[result['annotations'][i]['displayName']].append(result['annotations'][i]['textExtraction']['textSegment'])\n",
        "                truth_table[result['annotations'][i]['displayName']] = True\n",
        "        \n",
        "        # if any annotation not available for invoice them append empty value for same \n",
        "        for key, value in truth_table.items():\n",
        "            if value == False:\n",
        "                placeholder[key].append({})\n",
        "\n",
        "        #--------------------------------EXTRACTING TOKENS GIVEN BY OCR for EACH INVOICE and its bounding Polygon--------------------------\n",
        "        temp = {'tokensOfinvoice':[],'polygons':[]}\n",
        "        start_off = []\n",
        "        end_off = []\n",
        "\n",
        "        # extracting the raw text provided by OCR \n",
        "        for i in range(len(result['document']['layout'])):\n",
        "            temp['tokensOfinvoice'].append(result['document']['layout'][i]['textSegment']['content'])\n",
        "            temp['polygons'].append(result['document']['layout'][i]['boundingPoly']['normalizedVertices'])\n",
        "            if 'startOffset' in result['document']['layout'][i]['textSegment']:\n",
        "                start_off.append(int(result['document']['layout'][i]['textSegment']['startOffset']))\n",
        "            else:\n",
        "                start_off.append(0)\n",
        "            \n",
        "            if 'endOffset' in result['document']['layout'][i]['textSegment']:\n",
        "                end_off.append(int(result['document']['layout'][i]['textSegment']['endOffset']))\n",
        "            else: \n",
        "                end_off.append(-1)\n",
        "\n",
        "        tokens['sentTokens'].append(temp['tokensOfinvoice'])\n",
        "        tokens['boundingPoly'].append(temp['polygons'])\n",
        "        tokens['sentToken_startOffset'].append(start_off)\n",
        "        tokens['sentToken_endOffset'].append(end_off)\n",
        "\n",
        "\n",
        "# getting the list of all annotations offset and tokens ( removing first null entry )\n",
        "placeholder['Date'] = placeholder['Date'][1:]\n",
        "placeholder['Address']= placeholder['Address'][1:]\n",
        "placeholder['Name'] = placeholder['Name'][1:]\n",
        "placeholder['Amount'] = placeholder['Amount'][1:]\n",
        "\n",
        "placeholder['sentTokens'] = tokens['sentTokens'][1:]\n",
        "placeholder['sentToken_startOffset'] = tokens['sentToken_startOffset'][1:]\n",
        "placeholder['sentToken_endOffset'] = tokens['sentToken_endOffset'][1:]\n",
        "placeholder['boundingPoly'] = tokens['boundingPoly'][1:]\n",
        "# appending the raw text and type of invoice\n",
        "placeholder['text'] = documents\n",
        "placeholder['dataType'] = category\n",
        "\n",
        "\n",
        "# checking for count of all annotations and documents\n",
        "print(placeholder.keys())\n",
        "print(len(placeholder['Amount']))\n",
        "print(len(placeholder['Date']))\n",
        "print(len(placeholder['Address']))\n",
        "print(len(placeholder['Name']))\n",
        "print(len(placeholder['text']))\n",
        "print(len(placeholder['dataType']))\n",
        "\n",
        "data = pd.DataFrame.from_dict(placeholder)\n",
        "\n",
        "# Remember we have dictionary containing the start and end offset, we can extract them \n",
        "# separately too, if required.\n",
        "#--------------------------------------------------------------------------\n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']:\n",
        "    # sometimes startoffset might not be present if annotation start from first index of raw text\n",
        "    data['{}_start'.format(keys)] = data[keys].apply(lambda x: int(x['startOffset']) if 'startOffset' in x.keys()\n",
        "                        else ( 0 if 'endOffset' in x.keys() else None ))\n",
        "\n",
        "    # similarly as above endoffset might not be present if the annotations end is last index of raw text                 \n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']:   \n",
        "    data['{}_end'.format(keys)] = data[keys].apply(lambda x: int(x['endOffset']) if 'endOffset' in x.keys()\n",
        "                        else ( -1 if 'startOffset' in x.keys() else None))\n",
        "\n",
        "data = data.drop(['Date', 'Address', 'Name', 'Amount'], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n1Jcq7nPxZr"
      },
      "source": [
        "Lets contruct the target for the models, the OCR gives the startoffset and endoffset for each label, however this is at character. To make the encoding for each label at otken level, we will use simple logic if the startOffset of token lies within the bound of actual label then its part of label else not. This approach has limitation, because if the actual label is sub-part of token then this will exclude that token.This is one limitation of using the output of OCR. This same cause is also the reason for the AutoML bad performance, because any model will work on token level, however if the labelling is done by taking the sub-span within token then model is not able to take that into factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UhY6-bmPxZs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# placeholder to keep the target of each label\n",
        "import math\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "for i,j in placeholder.items():\n",
        "    for a,b,c,d in zip(data['sentToken_startOffset'],data['sentToken_endOffset'],data['{}_start'.format(i)], data['{}_end'.format(i)]):\n",
        "\n",
        "        if not math.isnan(c):\n",
        "\n",
        "            start_left = np.array(a) >=c\n",
        "            start_right = np.array(a) <=d\n",
        "            start_truth_table = start_left == start_right\n",
        "            \n",
        "            end_left = np.array(b)  >= c\n",
        "            end_right = np.array(b) <=d\n",
        "            end_truth_table = end_left == end_right\n",
        "            truth_table = start_truth_table + end_truth_table \n",
        "\n",
        "            placeholder['{}'.format(i)].append(truth_table.astype(int))\n",
        "        else:\n",
        "            placeholder['{}'.format(i)].append(list(np.zeros(len(a)).astype(int)))\n",
        "\n",
        "# removing the first null entry.\n",
        "placeholder['dateTarget'] = placeholder['Date'][1:]\n",
        "placeholder['addressTarget']= placeholder['Address'][1:]\n",
        "placeholder['nameTarget'] = placeholder['Name'][1:]\n",
        "placeholder['amountTarget'] = placeholder['Amount'][1:]\n",
        "\n",
        "del placeholder['Address'], placeholder['Amount'], placeholder['Date'], placeholder['Name']\n",
        "\n",
        "# creating final dataframe \n",
        "y = pd.DataFrame.from_dict(placeholder)\n",
        "data = pd.concat([data,y], axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGGTl7B_PxZs",
        "outputId": "a994024d-63cf-4c6c-b729-e9af2eb04395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of Date labels 946\n",
            "number of Name labels 947\n",
            "number of Address labels 949\n",
            "number of Amount labels 944\n"
          ]
        }
      ],
      "source": [
        "# If any token is present in label, then the the max value in the list for that sentence for that label will be 1, \n",
        "# hence if eveything is fine then we shoud see same numbers for all labels. However as highlighted due to slicing of token in case of amount, \n",
        "# it is have mis match. \n",
        "# Ex: if the token is RM170.00, now in labelling the actual value of amount is just 170.00, but since this value is sub-span of this whole token, \n",
        "# the encoding cannnot be done eeasily, therefore we to take care of such cases we take the whole token rather than just the sub-span.\n",
        "\n",
        "print('number of Date labels', sum([max(x) for x in data.dateTarget]))\n",
        "print('number of Name labels', sum([max(x) for x in data.nameTarget]))\n",
        "print('number of Address labels', sum([max(x) for x in data.addressTarget]))\n",
        "print('number of Amount labels', sum([max(x) for x in data.amountTarget]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br4q7TA2PxZt"
      },
      "source": [
        "Before we proceed lets invistigate if everything is okay in terms of data frame creation or not. We will do this by extracting the token from annotations encoding for each label and comparing with the actuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MqX4ZcC0Eo9"
      },
      "outputs": [],
      "source": [
        "## get the tokens set which form part of each label.\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.dateTarget) ]\n",
        "data['dateTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.nameTarget) ]\n",
        "data['nameTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.addressTarget) ]\n",
        "data['addressTokens'] = temp\n",
        "\n",
        "temp = [np.array(token)[np.array(mask == 1)] for (token, mask) in zip(data.sentTokens, data.amountTarget) ]\n",
        "data['amountTokens'] = temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6emR4RyqPxZu",
        "outputId": "d4c3522e-3f69-41d6-f636-cfde2760efaf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-474670a3-e4ec-46bc-a254-a93490898e64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentTokens</th>\n",
              "      <th>sentToken_startOffset</th>\n",
              "      <th>sentToken_endOffset</th>\n",
              "      <th>boundingPoly</th>\n",
              "      <th>text</th>\n",
              "      <th>dataType</th>\n",
              "      <th>Date_start</th>\n",
              "      <th>Address_start</th>\n",
              "      <th>Name_start</th>\n",
              "      <th>Amount_start</th>\n",
              "      <th>Date_end</th>\n",
              "      <th>Address_end</th>\n",
              "      <th>Name_end</th>\n",
              "      <th>Amount_end</th>\n",
              "      <th>dateTarget</th>\n",
              "      <th>addressTarget</th>\n",
              "      <th>nameTarget</th>\n",
              "      <th>amountTarget</th>\n",
              "      <th>dateTokens</th>\n",
              "      <th>nameTokens</th>\n",
              "      <th>addressTokens</th>\n",
              "      <th>amountTokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[RESTAURANT, SIN, DU, K3-113, ,, JL, IBRAHIM, ...</td>\n",
              "      <td>[0, 11, 15, 18, 24, 26, 29, 37, 44, 50, 56, 62...</td>\n",
              "      <td>[10, 14, 17, 24, 25, 28, 36, 43, 49, 55, 61, 6...</td>\n",
              "      <td>[[{'x': 0.33425927, 'y': 0.25343812}, {'x': 0....</td>\n",
              "      <td>RESTAURANT SIN DU\\nK3-113, JL IBRAHIM SULTAN\\n...</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>97.0</td>\n",
              "      <td>18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>107.0</td>\n",
              "      <td>67</td>\n",
              "      <td>17.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[09/03/2018]</td>\n",
              "      <td>[RESTAURANT, SIN, DU]</td>\n",
              "      <td>[K3-113, ,, JL, IBRAHIM, SULTAN, 80300, JOHOR,...</td>\n",
              "      <td>[170., 00]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[อ, ว, ), BECON, STATIONER, Becon, Enterprise,...</td>\n",
              "      <td>[0, 1, 2, 4, 10, 20, 26, 37, 41, 45, 46, 51, 5...</td>\n",
              "      <td>[1, 2, 3, 9, 19, 25, 36, 40, 44, 46, 51, 52, 5...</td>\n",
              "      <td>[[{'x': 0.8927039, 'y': 0.058606368}, {'x': 0....</td>\n",
              "      <td>อว)\\nBECON STATIONER\\nBecon Enterprise Sdn Bhd...</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>234.0</td>\n",
              "      <td>78</td>\n",
              "      <td>20.0</td>\n",
              "      <td>710.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>134</td>\n",
              "      <td>44.0</td>\n",
              "      <td>716.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[13-11-2017]</td>\n",
              "      <td>[Becon, Enterprise, Sdn, Bhd]</td>\n",
              "      <td>[No.41G, ,, Jln, SS21, /, 60, ,, Damansara, Ut...</td>\n",
              "      <td>[270.10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[GARDENIA, BAKERIES, (, KL, ), SDN, BHD, (, 13...</td>\n",
              "      <td>[0, 9, 18, 19, 21, 23, 27, 31, 32, 39, 43, 44,...</td>\n",
              "      <td>[8, 17, 19, 21, 22, 26, 30, 32, 38, 42, 44, 45...</td>\n",
              "      <td>[[{'x': 0.050324675, 'y': 0.07731093}, {'x': 0...</td>\n",
              "      <td>GARDENIA BAKERIES (KL) SDN BHD (139386\\nLot 3,...</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>212.0</td>\n",
              "      <td>39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>654.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>91</td>\n",
              "      <td>30.0</td>\n",
              "      <td>658.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[16/09/2017]</td>\n",
              "      <td>[GARDENIA, BAKERIES, (, KL, ), SDN, BHD]</td>\n",
              "      <td>[Lot, 3, ,, Jalan, Pelabur, 23.1, ,, 40300, Sh...</td>\n",
              "      <td>[3.21]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[MYDIN, TRI, SHAAS, SDN, BHD, (, 728515, -, M,...</td>\n",
              "      <td>[0, 6, 10, 16, 20, 24, 25, 31, 32, 33, 35, 41,...</td>\n",
              "      <td>[5, 9, 15, 19, 23, 25, 31, 32, 33, 34, 40, 45,...</td>\n",
              "      <td>[[{'x': 0.24736226, 'y': 0.04020497}, {'x': 0....</td>\n",
              "      <td>MYDIN\\nTRI SHAAS SDN BHD (728515-M)\\nMYDIN MAR...</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>245.0</td>\n",
              "      <td>55</td>\n",
              "      <td>6.0</td>\n",
              "      <td>736.0</td>\n",
              "      <td>254.0</td>\n",
              "      <td>128</td>\n",
              "      <td>23.0</td>\n",
              "      <td>741.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[4/02/2017]</td>\n",
              "      <td>[TRI, SHAAS, SDN, BHD]</td>\n",
              "      <td>[4-20, ,, Jalan, Ria, 25/62, Taman, Sri, Muda,...</td>\n",
              "      <td>[85.10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[99, SPEED, MART, S, /, B, (, 519537, -, X, ),...</td>\n",
              "      <td>[0, 3, 9, 14, 15, 16, 18, 19, 25, 26, 27, 29, ...</td>\n",
              "      <td>[2, 8, 13, 15, 16, 17, 19, 25, 26, 27, 28, 32,...</td>\n",
              "      <td>[[{'x': 0.187251, 'y': 0.21455939}, {'x': 0.22...</td>\n",
              "      <td>99 SPEED MART S/B (519537-X)\\nLOT P.T. 2811, J...</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>187.0</td>\n",
              "      <td>29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>261.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>93</td>\n",
              "      <td>17.0</td>\n",
              "      <td>266.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[19-03-18]</td>\n",
              "      <td>[99, SPEED, MART, S, /, B]</td>\n",
              "      <td>[LOT, P.T., 2811, ,, JALAN, ANGSA, ,, TAMAN, B...</td>\n",
              "      <td>[11.40]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>944</th>\n",
              "      <td>[SANYU, STATIONERY, SHOP, NO, ., 316, &amp;, 33G, ...</td>\n",
              "      <td>[0, 6, 17, 22, 24, 26, 29, 30, 33, 35, 41, 47,...</td>\n",
              "      <td>[5, 16, 21, 24, 25, 29, 30, 33, 34, 40, 46, 52...</td>\n",
              "      <td>[[{'x': 0.086677365, 'y': 0.08934481}, {'x': 0...</td>\n",
              "      <td>SANYU STATIONERY SHOP\\nNO. 316&amp;33G, JALAN SETI...</td>\n",
              "      <td>TEST</td>\n",
              "      <td>492.0</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>386.0</td>\n",
              "      <td>502.0</td>\n",
              "      <td>78</td>\n",
              "      <td>21.0</td>\n",
              "      <td>390.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[02/12/2017]</td>\n",
              "      <td>[SANYU, STATIONERY, SHOP]</td>\n",
              "      <td>[NO, ., 316, &amp;, 33G, ,, JALAN, SETIA, INDAH, X...</td>\n",
              "      <td>[8.70]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>[RESTORAN, WAN, SHENG, 002043319, -, W, No.2, ...</td>\n",
              "      <td>[0, 9, 13, 19, 28, 29, 31, 35, 37, 43, 54, 58,...</td>\n",
              "      <td>[8, 12, 18, 28, 29, 30, 35, 36, 42, 53, 58, 59...</td>\n",
              "      <td>[[{'x': 0.24928367, 'y': 0.14020029}, {'x': 0....</td>\n",
              "      <td>RESTORAN WAN SHENG\\n002043319-W\\nNo.2, Jalan T...</td>\n",
              "      <td>TEST</td>\n",
              "      <td>195.0</td>\n",
              "      <td>31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>457.0</td>\n",
              "      <td>205.0</td>\n",
              "      <td>116</td>\n",
              "      <td>18.0</td>\n",
              "      <td>461.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[19-03-2018]</td>\n",
              "      <td>[RESTORAN, WAN, SHENG]</td>\n",
              "      <td>[No.2, ,, Jalan, Temenggung, 19/9, ,, Seksyen,...</td>\n",
              "      <td>[9.10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>946</th>\n",
              "      <td>[PAID, 1, MAR, 2018, MR, ., D.I.Y, W, ), SDN, ...</td>\n",
              "      <td>[0, 5, 7, 11, 16, 18, 20, 26, 27, 29, 33, 37, ...</td>\n",
              "      <td>[4, 6, 10, 15, 18, 19, 25, 27, 28, 32, 36, 38,...</td>\n",
              "      <td>[[{'x': 0.57678354, 'y': 0.047028646}, {'x': 0...</td>\n",
              "      <td>PAID\\n1 MAR 2018\\nMR. D.I.Y W) SDN BHD\\n(CO Re...</td>\n",
              "      <td>TEST</td>\n",
              "      <td>5.0</td>\n",
              "      <td>55</td>\n",
              "      <td>16.0</td>\n",
              "      <td>476.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>150</td>\n",
              "      <td>36.0</td>\n",
              "      <td>481.0</td>\n",
              "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, MAR, 2018]</td>\n",
              "      <td>[MR, ., D.I.Y, W, ), SDN, BHD]</td>\n",
              "      <td>[LOT, 1951, -, A, &amp;, 1851-8, ,, JALAIKOD, 6, ,...</td>\n",
              "      <td>[37.10]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>947</th>\n",
              "      <td>[S, &amp;, Y, STATIONERY, (, 002050590, -, H, ), N...</td>\n",
              "      <td>[0, 1, 2, 4, 15, 16, 25, 26, 27, 29, 31, 33, 3...</td>\n",
              "      <td>[1, 2, 3, 14, 16, 25, 26, 27, 28, 31, 32, 36, ...</td>\n",
              "      <td>[[{'x': 0.3292517, 'y': 0.07223209}, {'x': 0.3...</td>\n",
              "      <td>S&amp;Y STATIONERY\\n(002050590-H)\\nNO. 36G JALAN B...</td>\n",
              "      <td>TEST</td>\n",
              "      <td>331.0</td>\n",
              "      <td>29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>586.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>122</td>\n",
              "      <td>14.0</td>\n",
              "      <td>591.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[05, -, Jan, -, 2017]</td>\n",
              "      <td>[S, &amp;, Y, STATIONERY]</td>\n",
              "      <td>[NO, ., 36G, JALAN, BULAN, BM, U5, /, BM, ,, B...</td>\n",
              "      <td>[72.00]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>[OE, AEON, CO, ., (, M, ), BHD, ., (, 126926, ...</td>\n",
              "      <td>[0, 3, 8, 10, 12, 13, 14, 16, 19, 21, 22, 28, ...</td>\n",
              "      <td>[2, 7, 10, 11, 13, 14, 15, 19, 20, 22, 28, 29,...</td>\n",
              "      <td>[[{'x': 0.7718795, 'y': -0.0033760972}, {'x': ...</td>\n",
              "      <td>OE\\nAEON CO. (M) BHD. (126926-H)\\n3 FLOOR, AEO...</td>\n",
              "      <td>TEST</td>\n",
              "      <td>455.0</td>\n",
              "      <td>32</td>\n",
              "      <td>3.0</td>\n",
              "      <td>320.0</td>\n",
              "      <td>465.0</td>\n",
              "      <td>113</td>\n",
              "      <td>20.0</td>\n",
              "      <td>324.0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[22/03/2018]</td>\n",
              "      <td>[AEON, CO, ., (, M, ), BHD, .]</td>\n",
              "      <td>[3, FLOOR, ,, AEON, TAMAN, MALURI, SC, JLN, JE...</td>\n",
              "      <td>[5.90]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>949 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-474670a3-e4ec-46bc-a254-a93490898e64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-474670a3-e4ec-46bc-a254-a93490898e64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-474670a3-e4ec-46bc-a254-a93490898e64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentTokens  ... amountTokens\n",
              "0    [RESTAURANT, SIN, DU, K3-113, ,, JL, IBRAHIM, ...  ...   [170., 00]\n",
              "1    [อ, ว, ), BECON, STATIONER, Becon, Enterprise,...  ...     [270.10]\n",
              "2    [GARDENIA, BAKERIES, (, KL, ), SDN, BHD, (, 13...  ...       [3.21]\n",
              "3    [MYDIN, TRI, SHAAS, SDN, BHD, (, 728515, -, M,...  ...      [85.10]\n",
              "4    [99, SPEED, MART, S, /, B, (, 519537, -, X, ),...  ...      [11.40]\n",
              "..                                                 ...  ...          ...\n",
              "944  [SANYU, STATIONERY, SHOP, NO, ., 316, &, 33G, ...  ...       [8.70]\n",
              "945  [RESTORAN, WAN, SHENG, 002043319, -, W, No.2, ...  ...       [9.10]\n",
              "946  [PAID, 1, MAR, 2018, MR, ., D.I.Y, W, ), SDN, ...  ...      [37.10]\n",
              "947  [S, &, Y, STATIONERY, (, 002050590, -, H, ), N...  ...      [72.00]\n",
              "948  [OE, AEON, CO, ., (, M, ), BHD, ., (, 126926, ...  ...       [5.90]\n",
              "\n",
              "[949 rows x 22 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# lets check the whole dataframe\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fxzhdf00NPR"
      },
      "outputs": [],
      "source": [
        "## Lets read the actual text which form part of each label\n",
        "# placeholder for annotations text\n",
        "placeholder = {'Date':['null'], 'Address': ['null'], 'Name':['null'], 'Amount':['null']}\n",
        "\n",
        "for keys in ['Date', 'Address', 'Name', 'Amount']: \n",
        "    # read annotation_start, annotation_end and text from data\n",
        "    for a,b,c in data[['{}_start'.format(keys), '{}_end'.format(keys), 'text']].itertuples(index=False):\n",
        "        # extracting the text\n",
        "        if not np.isnan(a) and  not np.isnan(b):\n",
        "            placeholder[keys].append(c[int(a):int(b)])\n",
        "        # if annotation not present then append None\n",
        "        else:\n",
        "            placeholder[keys].append(None)\n",
        "\n",
        "placeholder['dateTextactual'] = placeholder['Date'][1:]\n",
        "placeholder['addressTextactual'] = placeholder['Address'][1:]\n",
        "placeholder['nameTextactual'] = placeholder['Name'][1:]\n",
        "placeholder['amountTextactual'] = placeholder['Amount'][1:]\n",
        "\n",
        "del placeholder['Address'], placeholder['Amount'], placeholder['Date'], placeholder['Name']\n",
        "\n",
        "y = pd.DataFrame.from_dict(placeholder)\n",
        "\n",
        "###-------Lets construct the each annotations from tokens and targetencoding\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.dateTokens]\n",
        "y['dateFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.nameTokens]\n",
        "y['nameFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.addressTokens]\n",
        "y['addressFromTokens'] = temp\n",
        "\n",
        "temp = [' '.join([str(tokens) for tokens in list_])      for list_ in data.amountTokens]\n",
        "y['amountFromTokens'] = temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "oE28fCVC0QZa",
        "outputId": "57b356bb-ddb3-4738-c854-57a2d3264155"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d6611e54-18f8-46fe-81ac-5188392913a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dateTextactual</th>\n",
              "      <th>addressTextactual</th>\n",
              "      <th>nameTextactual</th>\n",
              "      <th>amountTextactual</th>\n",
              "      <th>dateFromTokens</th>\n",
              "      <th>nameFromTokens</th>\n",
              "      <th>addressFromTokens</th>\n",
              "      <th>amountFromTokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>09/03/2018</td>\n",
              "      <td>K3-113, JL IBRAHIM SULTAN\\n80300 JOHOR BAHRU\\n...</td>\n",
              "      <td>RESTAURANT SIN DU</td>\n",
              "      <td>170. 00</td>\n",
              "      <td>09/03/2018</td>\n",
              "      <td>RESTAURANT SIN DU</td>\n",
              "      <td>K3-113 , JL IBRAHIM SULTAN 80300 JOHOR BAHRU J...</td>\n",
              "      <td>170. 00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13-11-2017</td>\n",
              "      <td>No.41G, Jln SS21/60, Damansara\\nUtama, 47400 P...</td>\n",
              "      <td>Becon Enterprise Sdn Bhd</td>\n",
              "      <td>270.10</td>\n",
              "      <td>13-11-2017</td>\n",
              "      <td>Becon Enterprise Sdn Bhd</td>\n",
              "      <td>No.41G , Jln SS21 / 60 , Damansara Utama , 474...</td>\n",
              "      <td>270.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16/09/2017</td>\n",
              "      <td>Lot 3, Jalan Pelabur 23.1,\\n40300 Shah Alam, S...</td>\n",
              "      <td>GARDENIA BAKERIES (KL) SDN BHD</td>\n",
              "      <td>3.21</td>\n",
              "      <td>16/09/2017</td>\n",
              "      <td>GARDENIA BAKERIES ( KL ) SDN BHD</td>\n",
              "      <td>Lot 3 , Jalan Pelabur 23.1 , 40300 Shah Alam ,...</td>\n",
              "      <td>3.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/02/2017</td>\n",
              "      <td>4-20, Jalan Ria 25/62 Taman Sri Muda\\nSeksyen ...</td>\n",
              "      <td>TRI SHAAS SDN BHD</td>\n",
              "      <td>85.10</td>\n",
              "      <td>4/02/2017</td>\n",
              "      <td>TRI SHAAS SDN BHD</td>\n",
              "      <td>4-20 , Jalan Ria 25/62 Taman Sri Muda Seksyen ...</td>\n",
              "      <td>85.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19-03-18</td>\n",
              "      <td>LOT P.T. 2811, JALAN ANGSA,\\nTAMAN BERKELEY\\n4...</td>\n",
              "      <td>99 SPEED MART S/B</td>\n",
              "      <td>11.40</td>\n",
              "      <td>19-03-18</td>\n",
              "      <td>99 SPEED MART S / B</td>\n",
              "      <td>LOT P.T. 2811 , JALAN ANGSA , TAMAN BERKELEY 4...</td>\n",
              "      <td>11.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>944</th>\n",
              "      <td>02/12/2017</td>\n",
              "      <td>NO. 316&amp;33G, JALAN SETIA INDAH X ,U13/X\\n40170...</td>\n",
              "      <td>SANYU STATIONERY SHOP</td>\n",
              "      <td>8.70</td>\n",
              "      <td>02/12/2017</td>\n",
              "      <td>SANYU STATIONERY SHOP</td>\n",
              "      <td>NO . 316 &amp; 33G , JALAN SETIA INDAH X , U13 / X...</td>\n",
              "      <td>8.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>19-03-2018</td>\n",
              "      <td>No.2, Jalan Temenggung 19/9,\\nSeksyen 9, Banda...</td>\n",
              "      <td>RESTORAN WAN SHENG</td>\n",
              "      <td>9.10</td>\n",
              "      <td>19-03-2018</td>\n",
              "      <td>RESTORAN WAN SHENG</td>\n",
              "      <td>No.2 , Jalan Temenggung 19/9 , Seksyen 9 , Ban...</td>\n",
              "      <td>9.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>946</th>\n",
              "      <td>1 MAR 2018</td>\n",
              "      <td>LOT 1951-A &amp; 1851-8, JALAIKOD 6,\\nKAWASAN PERI...</td>\n",
              "      <td>MR. D.I.Y W) SDN BHD</td>\n",
              "      <td>37.10</td>\n",
              "      <td>1 MAR 2018</td>\n",
              "      <td>MR . D.I.Y W ) SDN BHD</td>\n",
              "      <td>LOT 1951 - A &amp; 1851-8 , JALAIKOD 6 , KAWASAN P...</td>\n",
              "      <td>37.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>947</th>\n",
              "      <td>05-Jan-2017</td>\n",
              "      <td>NO. 36G JALAN BULAN BM U5/BM,\\nBANDAR PINGGIRA...</td>\n",
              "      <td>S&amp;Y STATIONERY</td>\n",
              "      <td>72.00</td>\n",
              "      <td>05 - Jan - 2017</td>\n",
              "      <td>S &amp; Y STATIONERY</td>\n",
              "      <td>NO . 36G JALAN BULAN BM U5 / BM , BANDAR PINGG...</td>\n",
              "      <td>72.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>22/03/2018</td>\n",
              "      <td>3 FLOOR, AEON TAMAN MALURI SC\\nJLN JEJAKA, TAM...</td>\n",
              "      <td>AEON CO. (M) BHD.</td>\n",
              "      <td>5.90</td>\n",
              "      <td>22/03/2018</td>\n",
              "      <td>AEON CO . ( M ) BHD .</td>\n",
              "      <td>3 FLOOR , AEON TAMAN MALURI SC JLN JEJAKA , TA...</td>\n",
              "      <td>5.90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>949 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6611e54-18f8-46fe-81ac-5188392913a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6611e54-18f8-46fe-81ac-5188392913a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6611e54-18f8-46fe-81ac-5188392913a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    dateTextactual  ... amountFromTokens\n",
              "0       09/03/2018  ...          170. 00\n",
              "1       13-11-2017  ...           270.10\n",
              "2       16/09/2017  ...             3.21\n",
              "3        4/02/2017  ...            85.10\n",
              "4         19-03-18  ...            11.40\n",
              "..             ...  ...              ...\n",
              "944     02/12/2017  ...             8.70\n",
              "945     19-03-2018  ...             9.10\n",
              "946     1 MAR 2018  ...            37.10\n",
              "947    05-Jan-2017  ...            72.00\n",
              "948     22/03/2018  ...             5.90\n",
              "\n",
              "[949 rows x 8 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# just for demo..purpose\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GJfsAFJmHXc"
      },
      "source": [
        "Lets start building the model now. We will start with feature extraction. For this we will use the spacy to give features values for each token. \n",
        "\n",
        "> \n",
        "*   Feature1: index of token in the invoice\n",
        "*   Feature2: Is token alphanumeric or not\n",
        "*   Feature3: Whether Token is like Num or not ( its different from is-alpha Ex: 01-11189 will have like_Num =  True and is_alpha = False, but the token 21:15 will have like_Num = False and is_alpha = False, as the other is likely representing time and is not purely a Number)\n",
        "*   Feature4: length of token\n",
        "Lets have a look what kind of values we get one invoice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVuoqfKATMag"
      },
      "outputs": [],
      "source": [
        "##lets explore what kind of values we get, and what it means by these features\n",
        "\n",
        "# function to take the list of tokens for each invoice and return the features for each token\n",
        "\n",
        "def invoiceFeatures(sent,polygons): \n",
        "    idx = 0\n",
        "    temp1 = []\n",
        "    temp2 = []\n",
        "    for word, box in zip(sent, polygons):\n",
        "        box[0]\n",
        "        # as some token dont have x or y coordinate if it start from 0, therefore need to check for that\n",
        "        indices = [ checkMissingCoordinate(box[0]), checkMissingCoordinate(box[1]), checkMissingCoordinate(box[2]), checkMissingCoordinate(box[3])]\n",
        "        temp1.append(indices)\n",
        "        feat = [idx, word.isalpha(), word.isalnum(), len(word)]\n",
        "        temp2.append(feat)\n",
        "        idx +=1\n",
        "    temp1, temp2  = np.array(temp1), np.array(temp2)\n",
        "    return np.concatenate((temp1.reshape(temp1.shape[0],8) , temp2), axis = 1)\n",
        "\n",
        "# function to iterate over the all the invoice, this will call sent2features internally\n",
        "def prep_features(sents,polygonsList):\n",
        "    sent_features = []\n",
        "    \n",
        "    for sent,polygons in zip(sents, polygonsList):\n",
        "        temp = invoiceFeatures(sent, polygons)\n",
        "        temp = np.transpose(temp)\n",
        "        temp = tf.keras.preprocessing.sequence.pad_sequences(temp, padding='post', maxlen= max_len, value = -1, dtype= 'float32')\n",
        "        temp = np.transpose(temp)\n",
        "        # print(temp)\n",
        "        sent_features.append(temp)\n",
        "    return np.array(sent_features)\n",
        "\n",
        "def checkMissingCoordinate(coordinatePair):\n",
        "    temp = []\n",
        "\n",
        "    if 'x' in coordinatePair.keys():\n",
        "        temp.append(coordinatePair['x'])\n",
        "    else:\n",
        "        temp.append(0.0)\n",
        "    \n",
        "    if 'y' in coordinatePair.keys():\n",
        "        temp.append(coordinatePair['y'])\n",
        "    else: \n",
        "        temp.append(0.0)\n",
        "    \n",
        "    return temp\n",
        "\n",
        "  \n",
        "# this is just some tokenizing technique using keras and create the library of tokens...\n",
        "# lets build custom tokenizer adapted for this corpus\n",
        "\n",
        "def tokenize(lang, num_words = None):\n",
        "    # lang = list of sentences in a language\n",
        "\n",
        "    # lets select the default config and basic Tokenizer from keras\n",
        "    \n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', num_words= num_words)\n",
        "\n",
        "    # this step is necessary to allow tokenzier build its words id for the corpus \n",
        "    # from the lang = list of sentences\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "    ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "    ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "    ## and pads the sequences to match the longest sequences in the given input\n",
        "    # this will make all sentences of same length\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', value = 0)\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPCU2xthUPsW",
        "outputId": "8f053819-d9f8-48eb-e77c-143b709d9095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max length is 317\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# getting max length of sequence of tokens for invoices\n",
        "# we need this variable for padding\n",
        "\n",
        "max_len = max([len(x) for x in data.sentTokens])\n",
        "print(\"max length is {}\".format(max_len))\n",
        "\n",
        "# we will be dropping the data where any label is missing, this will be done in training and validation\n",
        "type_data = ['TRAIN', 'VALIDATION', 'TEST']\n",
        "d_train = data[data.dataType == 'TRAIN']\n",
        "d_valid =  data[data.dataType == 'VALIDATION']\n",
        "\n",
        "d_train = d_train.dropna(axis = 0, subset= ['Date_start','Name_start','Address_start','Amount_start','Date_end','Address_end','Name_end','Amount_end'])\n",
        "d_valid = d_valid.dropna(axis = 0, subset= ['Date_start','Name_start','Address_start','Amount_start','Date_end','Address_end','Name_end','Amount_end'])\n",
        "\n",
        "X_train_feat = prep_features(d_train.sentTokens, d_train.boundingPoly)\n",
        "X_valid_feat = prep_features(d_valid.sentTokens, d_valid.boundingPoly)\n",
        "X_test_feat = prep_features(data[data.dataType == 'TEST'].sentTokens, data[data.dataType == 'TEST'].boundingPoly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJwaB7Poc2Z8"
      },
      "outputs": [],
      "source": [
        "# getting token ids for each token and padding each invoice\n",
        "# Further we split the data (train, validation and test)\n",
        "# splitting the data\n",
        "\n",
        "X_train, tokenizer_ = tokenize(d_train.sentTokens)\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len, padding='post', value = 0)\n",
        "\n",
        "X_valid = tokenizer_.texts_to_sequences(d_valid.sentTokens)\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=max_len, padding='post', value = 0)\n",
        "\n",
        "X_test = tokenizer_.texts_to_sequences(data[data.dataType == 'TEST'].sentTokens)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len, padding='post', value = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEeHWtoxh5lv",
        "outputId": "43d7793c-7e1b-421e-d271-bbe5814b572e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_TRAIN (750, 317)\n",
            "Train_features (750, 317, 12)\n",
            "X_VALID (93, 317)\n",
            "Valid_features (93, 317, 12)\n"
          ]
        }
      ],
      "source": [
        "# lets have a look at the shapes of data\n",
        "\n",
        "print(\"X_TRAIN\",X_train.shape)\n",
        "print(\"Train_features\",X_train_feat.shape)\n",
        "print(\"X_VALID\",X_valid.shape)\n",
        "print(\"Valid_features\",X_valid_feat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQzH8cYy4SkA"
      },
      "outputs": [],
      "source": [
        "# this is required as to tell the input shape to model.\n",
        "shape_ = X_train_feat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpKUOyItzKZc"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "  # we will create two inputs one which has sequence of just tokens and\n",
        "  # other which has feature of each token (so array of features of each token)\n",
        "  x1 = Input(shape=(max_len,))\n",
        "  x2 = Input(shape = (max_len,X_train_feat.shape[2]))\n",
        "#----------------------------------------TOKEN Embedding -----------------------------------\n",
        "\n",
        "  mask1 = layers.Masking(mask_value=0)(x1)\n",
        "  embed = layers.Embedding(input_dim= len(tokenizer_.word_index), output_dim=128, input_length=max_len)(mask1)\n",
        "  norm1 = layers.BatchNormalization(axis=1)(embed)\n",
        "\n",
        "#---------------------------------------------FEATURE-----------------------------\n",
        "\n",
        "  mask2 = layers.Masking(mask_value=-1)(x2)\n",
        "#----------------------------------------------Merging both---NOW first we pass through memory(LSTM) and Dense(for more complex fetaure)......\n",
        "\n",
        "  merge = layers.concatenate([norm1,mask2])\n",
        "  drop1 = layers.Dropout(0.2)(merge)\n",
        "  lstm1 = layers.LSTM(units= 300, return_sequences= True)(drop1)\n",
        "  denseLinear = layers.Dense(units= 300, activation = 'relu')(lstm1)\n",
        "# ------------------------ and now attention starts ( attention acts on memory (LSTM)-----------------------\n",
        "  attention = layers.Dense(1, activation='tanh')(lstm1)\n",
        "  attention = layers.Flatten()(attention)\n",
        "  attention = layers.Activation('softmax')(attention)\n",
        "  attention = layers.RepeatVector(300)(attention)\n",
        "  attention = layers.Permute([2, 1])(attention)\n",
        "# -------------------------------------------------Attention tells LSTM what to remember and what to forget---------------\n",
        "  sent_representation = layers.Multiply()([lstm1, attention])\n",
        "\n",
        "  dense = layers.Dense(512, activation = 'relu')(sent_representation)\n",
        "  drop1 = layers.Dropout(0.3)(dense)\n",
        "  dense2 = layers.Dense(128)(drop1)\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "#   # KEEP THIS INTACT\n",
        "  predictions = layers.Dense(5, activation='softmax')(dense2)\n",
        "\n",
        "  model = Model(inputs = ([x1,x2]), outputs = predictions)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI0b4IrVSfFq"
      },
      "outputs": [],
      "source": [
        "# since now we ahve all annotations together we create target encoding for same\n",
        "\n",
        "def createTargetForMultilabel():\n",
        "   # with this now 0: No label, 1: name, 2: address, 3: date, 4: amount\n",
        "  d_train['tags'] = d_train.nameTarget + d_train.addressTarget*2 + d_train.dateTarget*3 + d_train.amountTarget*4 \n",
        "  a = d_train.tags.tolist()\n",
        "  y_train = tf.keras.preprocessing.sequence.pad_sequences(a, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "\n",
        "  d_valid['tags'] = d_valid.nameTarget + d_valid.addressTarget*2 + d_valid.dateTarget*3 +  d_valid.amountTarget*4\n",
        "  a = d_valid.tags.tolist()\n",
        "  y_valid = tf.keras.preprocessing.sequence.pad_sequences(a, padding='post', maxlen= max_len,dtype='int32', value = 0)\n",
        "\n",
        "# one hot encode target\n",
        "  y_train = [tf.keras.utils.to_categorical(i, num_classes=5) for i in y_train]\n",
        "  y_valid = [tf.keras.utils.to_categorical(i, num_classes=5) for i in y_valid]\n",
        "\n",
        "  y_train = np.array(y_train)\n",
        "  y_valid = np.array(y_valid)\n",
        "  return y_train, y_valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiAuDs9B1ua7"
      },
      "outputs": [],
      "source": [
        "# function to plot the loss, precision and recall, \n",
        "# caution code block has lines to save the files to drive.\n",
        "\n",
        "def display_history(history,name =\"None\"):\n",
        "\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import numpy as np                                        \n",
        "    # notebook and use it by doing slight modification\n",
        "    # \"\"\"Summarize history for accuracy and loss.\n",
        "\n",
        "    sns.set_palette(\"pastel\")\n",
        "    sns.set(style=\"darkgrid\")\n",
        "    path = '/content/drive/MyDrive/ING/'\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Loss_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['loss'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_loss'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('loss')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig(path + title + '.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Precision_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['precision'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_precision'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('precision')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig( path + title + '.png')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,6),sharey='row')\n",
        "    title = \"Recall_for_{}\".format(name)\n",
        "    sns.lineplot(data=history.history['recall'], ax = ax)\n",
        "    sns.lineplot(data=history.history['val_recall'], ax = ax)\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('recall')\n",
        "    plt.legend(['train', 'valid'], loc='upper left')\n",
        "    plt.title(title)\n",
        "    fig.savefig( path + title + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw8oLIBaZXF7"
      },
      "outputs": [],
      "source": [
        "# 0: None, 1: name, 2: address, 3: date, 4: amount\n",
        "def weighted_loss(class_weights):\n",
        "    weights = list(class_weights.values())\n",
        "    def loss(y_true, y_pred):\n",
        "\n",
        "        y_pred = be.clip(y_pred, be.epsilon(), 1 - be.epsilon())\n",
        "        \n",
        "        loss = y_true * be.log(y_pred) * weights\n",
        "        loss = -be.sum(loss, axis= -1)\n",
        "        return loss\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZYQAIWkRuyT"
      },
      "outputs": [],
      "source": [
        "# this will take the tag = 'date'/'name'...make the target encoding as per that and load and compile the model\n",
        "def LoadandCompile(tag):\n",
        "  be.clear_session()\n",
        "\n",
        "  y_train, y_valid =  createTargetForMultilabel()\n",
        "  class_weights = {0:0.05, 1:100, 2:20, 3: 200, 4: 180}\n",
        "  loss = weighted_loss(class_weights = class_weights)\n",
        "\n",
        "  optimizer=RectifiedAdam(learning_rate = 0.001, warmup_proportion = 0.3, beta_1 = 0.9,weight_decay= 0.05,   \n",
        "                                            total_steps= 2500, min_lr= 0.0001)\n",
        "  model = build_model()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss= loss,\n",
        "                metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
        "  \n",
        "  return model, y_train, y_valid  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aMwnYCdXQAq"
      },
      "outputs": [],
      "source": [
        "# lets run the model for each annotation\n",
        "\n",
        "be.clear_session()\n",
        "\n",
        "model,y_train, y_valid = LoadandCompile('combined')\n",
        "\n",
        "print(\"Starting training {} on with model presented above\".format('combined'))\n",
        "history = model.fit(x=[X_train,X_train_feat], y=y_train,\n",
        "                  validation_data=([X_valid,X_valid_feat], y_valid),\n",
        "                  epochs=1200,batch_size=32) \n",
        "\n",
        "display_history(history, name = 'combined')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ebIypruIJNb",
        "outputId": "1c90e44c-79d8-40d3-bce0-f4dbd0d6dad9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/MyDrive/ING/{}_combined.h5'.format('attention'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL6GUJEFa-eS"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/ING/Attention/final/attention_combined_v3.h5', compile = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mkoBzgDmAi5",
        "outputId": "4abd6c17-6d8c-4fc1-bf52-f0742bbb1fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 317)]        0           []                               \n",
            "                                                                                                  \n",
            " masking (Masking)              (None, 317)          0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 317, 128)     1748224     ['masking[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 317, 12)]    0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 317, 128)    512         ['embedding[0][0]']              \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " masking_1 (Masking)            (None, 317, 12)      0           ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 317, 140)     0           ['batch_normalization[0][0]',    \n",
            "                                                                  'masking_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 317, 140)     0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 317, 300)     529200      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 317, 300)     90300       ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 317, 1)       301         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 317)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 317)          0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVector)   (None, 300, 317)     0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 317, 300)     0           ['repeat_vector[0][0]']          \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 317, 300)     0           ['lstm[0][0]',                   \n",
            "                                                                  'permute[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 317, 512)     154112      ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 317, 512)     0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 317, 128)     65664       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 317, 5)       645         ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,588,958\n",
            "Trainable params: 2,588,702\n",
            "Non-trainable params: 256\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z86QzPW3jBp0"
      },
      "outputs": [],
      "source": [
        "# lets do the prediction on test now and see the metrics\n",
        "a = model.predict([X_test,X_test_feat])\n",
        "\n",
        "b = a.tolist()\n",
        "df = data[data.dataType == 'TEST'].reset_index()\n",
        "df['probs'] = b\n",
        "def f(x,y,temp):\n",
        "  return(y[:len(x),temp])\n",
        "\n",
        "df['predictions'] = df.probs.apply(lambda x: (np.array(x) == np.array(x).max(axis =1)[:, None]).astype(int))\n",
        "df['predictedname'] = df.apply(lambda x: f(x.sentTokens,x.predictions,1), axis = 1)\n",
        "df['predictedaddress'] = df.apply(lambda x: f(x.sentTokens,x.predictions,2), axis = 1)\n",
        "df['predicteddate'] = df.apply(lambda x: f(x.sentTokens,x.predictions,3), axis = 1)\n",
        "df['predictedamount'] = df.apply(lambda x: f(x.sentTokens,x.predictions,4), axis = 1)\n",
        "\n",
        "\n",
        "#metric evlauation function\n",
        "def onehotinvoicesmetric(tag):\n",
        "  TP = 0\n",
        "  for a,b in zip(df['{}Target'.format(tag)],df['predicted{}'.format(tag)]):\n",
        "    count = (a == b)\n",
        "    if type(count) == type(True):\n",
        "      TP += 1\n",
        "    elif sum(count) == len(a):\n",
        "      TP += 1\n",
        "  return (TP/len(df))*100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXGi1zVDXgLv",
        "outputId": "a8188257-eaff-496e-eca4-53cfb3456475"
      },
      "outputs": [],
      "source": [
        "print('DATE',onehotinvoicesmetric('date'))\n",
        "print('AMOUNT', onehotinvoicesmetric('amount'))\n",
        "print('NAME', onehotinvoicesmetric('name'))\n",
        "print('ADDRESS',onehotinvoicesmetric('address'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AttentionModel.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3472910ebd2aa62b6c54b483bd66ffdd88459b69fe7bc051f28b07b9e0034dc5"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('tf_keras': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
